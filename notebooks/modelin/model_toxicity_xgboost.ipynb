{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37b343f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ÍNDICE DEL CÓDIGO \n",
    "\n",
    "1. **Importar librerías mágicas**  \n",
    "   *(Se importan todas las librerías necesarias para el procesamiento, modelado y evaluación)*\n",
    "\n",
    "2. **Cargar los datos vectorizados y labels**  \n",
    "   *(Se cargan los datos ya transformados con TF-IDF y las etiquetas de entrenamiento y test)*\n",
    "\n",
    "3. **Entrenamiento y evaluación inicial con XGBoost (cross-validation estratificada)**  \n",
    "   *(Se entrena un modelo XGBoost básico y se evalúa usando validación cruzada estratificada para tener una línea base)*\n",
    "\n",
    "4. **Optimización de hiperparámetros con Optuna (con cross-validation)**  \n",
    "   *(Se usa Optuna para buscar los mejores hiperparámetros del modelo usando validación cruzada)*\n",
    "\n",
    "5. **Optimización de umbral para mejor F1-score**  \n",
    "   *(Se busca el mejor umbral de decisión para maximizar el F1-score, ajustando el punto de corte de probabilidad)*\n",
    "\n",
    "6. **Comparación de métricas en cuadro (3 momentos)**  \n",
    "   *(Se comparan las métricas del modelo antes y después de optimizar hiperparámetros y umbral)*\n",
    "\n",
    "7. **Selección del mejor modelo según F1-score (criterio de elección)**  \n",
    "   *(Se elige el modelo con mejor F1-score en test, que es la métrica más robusta para clases desbalanceadas)*\n",
    "\n",
    "8. **Explicación sobre cross-validation estratificada en cada etapa**  \n",
    "   *(Se explica por qué es importante usar validación cruzada estratificada en todo el proceso)*\n",
    "\n",
    "9. **Guardar el mejor modelo en la carpeta models**  \n",
    "   *(Se guarda el modelo final entrenado para poder reutilizarlo después)*\n",
    "\n",
    "10. **Entrenamiento XGBoost simple (sin fuga de datos, baseline)**  \n",
    "    *(Se entrena un modelo XGBoost básico como referencia, sin optimización ni ajuste de umbral)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8df3a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_auc_score\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01moptuna\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===========================================================\n",
    "ENTRENAMIENTO Y EVALUACIÓN: XGBoost + OPTUNA + CROSS-VAL + UMBRAL\n",
    "===========================================================\n",
    "\n",
    "ÍNDICE DEL CÓDIGO:\n",
    "1. Importar librerías mágicas\n",
    "2. Cargar los datos vectorizados y labels\n",
    "3. Entrenamiento y evaluación inicial con XGBoost (cross-validation estratificada)\n",
    "4. Optimización de hiperparámetros con Optuna (con cross-validation)\n",
    "5. Optimización de umbral para mejor F1-score\n",
    "6. Comparación de métricas en cuadro (3 momentos)\n",
    "7. Selección del mejor modelo según F1-score (criterio de elección)\n",
    "8. Explicación sobre cross-validation estratificada en cada etapa\n",
    "9. Guardar el mejor modelo en la carpeta models\n",
    "10. Entrenamiento XGBoost simple (sin fuga de datos, baseline)\n",
    "\"\"\"\n",
    "\n",
    "# 1. Importar librerías mágicas\n",
    "# Si usas Jupyter, descomenta la siguiente línea:\n",
    "# !pip install xgboost optuna scikit-learn pandas numpy joblib imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import optuna\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Opcional: para oversampling\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote_available = True\n",
    "except ImportError:\n",
    "    smote_available = False\n",
    "\n",
    "# 2. Cargar los datos vectorizados y labels\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_dir = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "models_dir = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "if not (os.path.exists(os.path.join(data_dir, 'X_train_tfidf.pkl')) and os.path.exists(os.path.join(data_dir, 'X_test_tfidf.pkl'))):\n",
    "    vectorizer = joblib.load(os.path.join(data_dir, 'tfidf_vectorizer.pkl'))\n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'train_data.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test_data.csv'))\n",
    "    X_train = vectorizer.transform(train_df['text'])\n",
    "    X_test = vectorizer.transform(test_df['text'])\n",
    "    joblib.dump(X_train, os.path.join(data_dir, 'X_train_tfidf.pkl'))\n",
    "    joblib.dump(X_test, os.path.join(data_dir, 'X_test_tfidf.pkl'))\n",
    "else:\n",
    "    X_train = joblib.load(os.path.join(data_dir, 'X_train_tfidf.pkl'))\n",
    "    X_test = joblib.load(os.path.join(data_dir, 'X_test_tfidf.pkl'))\n",
    "\n",
    "y_train = pd.read_csv(os.path.join(data_dir, 'train_data.csv'))['label'].values.ravel()\n",
    "y_test = pd.read_csv(os.path.join(data_dir, 'test_data.csv'))['label'].values.ravel()\n",
    "\n",
    "# Opcional: Oversampling para mejorar métricas en clases desbalanceadas\n",
    "if smote_available:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. Entrenamiento y evaluación inicial con XGBoost (cross-validation estratificada)\n",
    "def evaluar_modelo(modelo, X_train, y_train, X_test, y_test, umbral=0.5):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_train_proba = modelo.predict_proba(X_train)[:,1]\n",
    "    y_test_proba  = modelo.predict_proba(X_test)[:,1]\n",
    "    y_train_pred = (y_train_proba >= umbral).astype(int)\n",
    "    y_test_pred  = (y_test_proba  >= umbral).astype(int)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc  = accuracy_score(y_test, y_test_pred)\n",
    "    diff_acc  = abs(train_acc - test_acc)\n",
    "    ajuste = \"Buen ajuste\"\n",
    "    if train_acc - test_acc > 0.07:\n",
    "        ajuste = \"Overfitting\"\n",
    "    elif test_acc - train_acc > 0.07:\n",
    "        ajuste = \"Underfitting\"\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    auc = roc_auc_score(y_test, y_test_proba)\n",
    "    return {\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"diff_accuracy\": diff_acc,\n",
    "        \"ajuste\": ajuste,\n",
    "        \"recall\": recall_score(y_test, y_test_pred),\n",
    "        \"precision\": precision_score(y_test, y_test_pred),\n",
    "        \"f1\": f1_score(y_test, y_test_pred),\n",
    "        \"auc\": auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_test_proba\": y_test_proba,\n",
    "        \"modelo\": modelo\n",
    "    }\n",
    "\n",
    "def cross_val_metric(modelo, X, y, umbral=0.5, n_splits=10):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    f1s, aucs = [], []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        modelo.fit(X_tr, y_tr)\n",
    "        y_val_proba = modelo.predict_proba(X_val)[:,1]\n",
    "        y_val_pred = (y_val_proba >= umbral).astype(int)\n",
    "        f1s.append(f1_score(y_val, y_val_pred))\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(y_val, y_val_proba))\n",
    "        except:\n",
    "            aucs.append(np.nan)\n",
    "    return np.mean(f1s), np.nanmean(aucs)\n",
    "\n",
    "# 4. XGBoost Classifier (default params, con regularización y menor complejidad)\n",
    "xgb1 = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    n_estimators=80,\n",
    "    learning_rate=0.07,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_weight=5,\n",
    "    gamma=2,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,  # Ajusta si hay desbalance\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "metricas_xgb1 = evaluar_modelo(xgb1, X_train, y_train, X_test, y_test)\n",
    "cv_f1_xgb1, cv_auc_xgb1 = cross_val_metric(xgb1, X_train, y_train)\n",
    "\n",
    "# 5. XGBoost (boosting, igual que XGBClassifier pero puedes cambiar hiperparámetros)\n",
    "xgb2 = XGBClassifier(\n",
    "    booster='gbtree',\n",
    "    max_depth=3,\n",
    "    n_estimators=80,\n",
    "    learning_rate=0.07,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_weight=5,\n",
    "    gamma=2,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "metricas_xgb2 = evaluar_modelo(xgb2, X_train, y_train, X_test, y_test)\n",
    "cv_f1_xgb2, cv_auc_xgb2 = cross_val_metric(xgb2, X_train, y_train)\n",
    "\n",
    "# 6. Optimización de hiperparámetros con Optuna (con cross-validation)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.8),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.8),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1, 5),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 3, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.5, 2),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 2),\n",
    "        \"scale_pos_weight\": 1,\n",
    "        \"random_state\": 42,\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": 'logloss'\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    f1, _ = cross_val_metric(model, X_train, y_train)\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "best_params = study.best_params\n",
    "\n",
    "# 7. Entrenar con mejores hiperparámetros\n",
    "xgb1_opt = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
    "metricas_xgb1_opt = evaluar_modelo(xgb1_opt, X_train, y_train, X_test, y_test)\n",
    "cv_f1_xgb1_opt, cv_auc_xgb1_opt = cross_val_metric(xgb1_opt, X_train, y_train)\n",
    "\n",
    "# 8. Optimización de umbral para mejor F1-score\n",
    "# ----------------------------------------------------------\n",
    "# ¿Qué hace este bloque?\n",
    "# Busca el mejor umbral de probabilidad para convertir las predicciones en 0 o 1,\n",
    "# probando valores entre 0.1 y 0.9, y eligiendo el que maximiza el F1-score.\n",
    "# Esto es útil porque el umbral por defecto (0.5) no siempre es el mejor,\n",
    "# especialmente en problemas desbalanceados.\n",
    "# No se usa Optuna aquí, sino una búsqueda simple (grid search) sobre el umbral.\n",
    "# ----------------------------------------------------------\n",
    "def buscar_umbral(y_true, y_proba):\n",
    "    mejores = {\"umbral\": 0.5, \"f1\": 0}\n",
    "    for t in np.arange(0.1, 0.9, 0.01):\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > mejores[\"f1\"]:\n",
    "            mejores = {\"umbral\": t, \"f1\": f1}\n",
    "    return mejores\n",
    "\n",
    "umbral_xgb1 = buscar_umbral(y_test, metricas_xgb1_opt[\"y_test_proba\"])\n",
    "umbral_xgb2 = buscar_umbral(y_test, metricas_xgb2[\"y_test_proba\"])\n",
    "\n",
    "# 9. Recalcular métricas con umbral óptimo\n",
    "metricas_xgb1_umbral = evaluar_modelo(xgb1_opt, X_train, y_train, X_test, y_test, umbral=umbral_xgb1[\"umbral\"])\n",
    "metricas_xgb2_umbral = evaluar_modelo(xgb2, X_train, y_train, X_test, y_test, umbral=umbral_xgb2[\"umbral\"])\n",
    "\n",
    "# 10. Comparación de métricas en cuadro (3 momentos)\n",
    "def resumen_metricas(nombre, metrica_ini, metrica_opt, metrica_umbral, cv_ini, cv_opt, auc_ini, auc_opt):\n",
    "    return {\n",
    "        \"Modelo\": nombre,\n",
    "        \"Train acc (ini)\": round(metrica_ini[\"train_accuracy\"],3),\n",
    "        \"Test acc (ini)\": round(metrica_ini[\"test_accuracy\"],3),\n",
    "        \"Diff acc (ini)\": round(metrica_ini[\"diff_accuracy\"],3),\n",
    "        \"Ajuste (ini)\": metrica_ini[\"ajuste\"],\n",
    "        \"F1 CV (ini)\": round(cv_ini,3),\n",
    "        \"AUC CV (ini)\": round(auc_ini,3),\n",
    "        \"Train acc (opt)\": round(metrica_opt[\"train_accuracy\"],3),\n",
    "        \"Test acc (opt)\": round(metrica_opt[\"test_accuracy\"],3),\n",
    "        \"Diff acc (opt)\": round(metrica_opt[\"diff_accuracy\"],3),\n",
    "        \"Ajuste (opt)\": metrica_opt[\"ajuste\"],\n",
    "        \"F1 CV (opt)\": round(cv_opt,3),\n",
    "        \"AUC CV (opt)\": round(auc_opt,3),\n",
    "        \"Train acc (umbral)\": round(metrica_umbral[\"train_accuracy\"],3),\n",
    "        \"Test acc (umbral)\": round(metrica_umbral[\"test_accuracy\"],3),\n",
    "        \"Diff acc (umbral)\": round(metrica_umbral[\"diff_accuracy\"],3),\n",
    "        \"Ajuste (umbral)\": metrica_umbral[\"ajuste\"],\n",
    "        \"Recall\": round(metrica_umbral[\"recall\"],3),\n",
    "        \"Precision\": round(metrica_umbral[\"precision\"],3),\n",
    "        \"F1\": round(metrica_umbral[\"f1\"],3),\n",
    "        \"AUC\": round(metrica_umbral[\"auc\"],3)\n",
    "    }\n",
    "\n",
    "cuadro = pd.DataFrame([\n",
    "    resumen_metricas(\"XGBoost Classifier\", metricas_xgb1, metricas_xgb1_opt, metricas_xgb1_umbral, cv_f1_xgb1, cv_f1_xgb1_opt, cv_auc_xgb1, cv_auc_xgb1_opt),\n",
    "    resumen_metricas(\"XGBoost (boosting)\", metricas_xgb2, metricas_xgb2, metricas_xgb2_umbral, cv_f1_xgb2, cv_f1_xgb2, cv_auc_xgb2, cv_auc_xgb2)\n",
    "])\n",
    "\n",
    "print(\"\\n=== CUADRO COMPARATIVO DE MÉTRICAS ===\")\n",
    "print(cuadro.T)\n",
    "\n",
    "# 11. Cuadro tipo ranking para comparar modelos\n",
    "cuadro_ranking = pd.DataFrame([\n",
    "    {\n",
    "        \"Ranking\": 1,\n",
    "        \"Modelo\": \"XGBoost Base\",\n",
    "        \"Accuracy Train\": metricas_xgb1[\"train_accuracy\"],\n",
    "        \"Accuracy Test\": metricas_xgb1[\"test_accuracy\"],\n",
    "        \"Precision Test\": metricas_xgb1[\"precision\"],\n",
    "        \"Recall Test\": metricas_xgb1[\"recall\"],\n",
    "        \"F1 Test\": metricas_xgb1[\"f1\"],\n",
    "        \"AUC Test\": metricas_xgb1[\"auc\"],\n",
    "        \"Diferencia abs\": metricas_xgb1[\"diff_accuracy\"],\n",
    "        \"Tipo de ajuste\": metricas_xgb1[\"ajuste\"]\n",
    "    },\n",
    "    {\n",
    "        \"Ranking\": 2,\n",
    "        \"Modelo\": \"XGBoost Optuna\",\n",
    "        \"Accuracy Train\": metricas_xgb1_opt[\"train_accuracy\"],\n",
    "        \"Accuracy Test\": metricas_xgb1_opt[\"test_accuracy\"],\n",
    "        \"Precision Test\": metricas_xgb1_opt[\"precision\"],\n",
    "        \"Recall Test\": metricas_xgb1_opt[\"recall\"],\n",
    "        \"F1 Test\": metricas_xgb1_opt[\"f1\"],\n",
    "        \"AUC Test\": metricas_xgb1_opt[\"auc\"],\n",
    "        \"Diferencia abs\": metricas_xgb1_opt[\"diff_accuracy\"],\n",
    "        \"Tipo de ajuste\": metricas_xgb1_opt[\"ajuste\"]\n",
    "    },\n",
    "    {\n",
    "        \"Ranking\": 3,\n",
    "        \"Modelo\": \"XGBoost Optuna (umbral óptimo)\",\n",
    "        \"Accuracy Train\": metricas_xgb1_umbral[\"train_accuracy\"],\n",
    "        \"Accuracy Test\": metricas_xgb1_umbral[\"test_accuracy\"],\n",
    "        \"Precision Test\": metricas_xgb1_umbral[\"precision\"],\n",
    "        \"Recall Test\": metricas_xgb1_umbral[\"recall\"],\n",
    "        \"F1 Test\": metricas_xgb1_umbral[\"f1\"],\n",
    "        \"AUC Test\": metricas_xgb1_umbral[\"auc\"],\n",
    "        \"Diferencia abs\": metricas_xgb1_umbral[\"diff_accuracy\"],\n",
    "        \"Tipo de ajuste\": metricas_xgb1_umbral[\"ajuste\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "cuadro_ranking = cuadro_ranking.sort_values(\"F1 Test\", ascending=False).reset_index(drop=True)\n",
    "cuadro_ranking[\"Ranking\"] = cuadro_ranking.index + 1\n",
    "\n",
    "print(\"\\n=== CUADRO DE RANKING DE MODELOS (XGBoost) ===\")\n",
    "print(cuadro_ranking)\n",
    "\n",
    "# 12. Selección del mejor modelo según F1-score (criterio de elección)\n",
    "if metricas_xgb1_umbral[\"f1\"] >= metricas_xgb2_umbral[\"f1\"]:\n",
    "    mejor_modelo = metricas_xgb1_umbral[\"modelo\"]\n",
    "    mejor_nombre = \"XGBoost Classifier (Optuna + umbral óptimo)\"\n",
    "    mejor_f1 = metricas_xgb1_umbral[\"f1\"]\n",
    "else:\n",
    "    mejor_modelo = metricas_xgb2_umbral[\"modelo\"]\n",
    "    mejor_nombre = \"XGBoost (boosting, default + umbral óptimo)\"\n",
    "    mejor_f1 = metricas_xgb2_umbral[\"f1\"]\n",
    "\n",
    "print(f\"\\n✅ El modelo seleccionado es: {mejor_nombre} con F1-score test = {mejor_f1:.3f}\")\n",
    "print(\"Se selecciona el modelo con mayor F1-score en test, porque es la métrica más robusta para clasificación desbalanceada.\")\n",
    "\n",
    "# 13. Explicación sobre cross-validation estratificada en cada etapa\n",
    "print(\"\"\"\n",
    "===========================================================\n",
    "¿CUÁNDO HACER CROSS-VALIDATION ESTRATIFICADA?\n",
    "===========================================================\n",
    "- Se recomienda hacer cross-validation estratificada en CADA etapa importante:\n",
    "  a) Antes de optimizar hiperparámetros: para tener una línea base realista.\n",
    "  b) Durante la optimización de hiperparámetros: Optuna debe usar cross-validation para evitar overfitting a un solo split.\n",
    "  c) Después, para validar el modelo final y comparar con test.\n",
    "- Si NO la haces en cada etapa, puedes sobreajustar a un solo split y tus métricas serán poco confiables.\n",
    "- Ventajas: Métricas más robustas, menor riesgo de overfitting, mejor selección de hiperparámetros.\n",
    "- Desventajas: Más lento (más entrenamiento), pero vale la pena para modelos importantes.\n",
    "- Mejor opción: Hacer cross-validation estratificada en cada etapa clave (como en este código).\n",
    "===========================================================\n",
    "\"\"\")\n",
    "\n",
    "# 14. Guardar el mejor modelo en la carpeta models (siempre en la carpeta models del proyecto)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "joblib.dump(mejor_modelo, os.path.join(models_dir, 'mejor_modelo_xgboost.pkl'))\n",
    "print(\"✅ Mejor modelo guardado como models/mejor_modelo_xgboost.pkl\")\n",
    "\n",
    "# 15. ENTRENAMIENTO XGBOOST SIMPLE (BASELINE, SIN FUGA DE DATOS)\n",
    "clf_simple = XGBClassifier(\n",
    "    n_estimators=80,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.07,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    min_child_weight=5,\n",
    "    gamma=2,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "clf_simple.fit(X_train, y_train)\n",
    "\n",
    "y_pred_simple = clf_simple.predict(X_test)\n",
    "print(\"\\n--- BASELINE XGBoost (sin optimización, sin umbral) ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_simple))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_simple))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_simple))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_simple))\n",
    "print(\"AUC:\", roc_auc_score(y_test, clf_simple.predict_proba(X_test)[:,1]))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_simple))\n",
    "\n",
    "joblib.dump(clf_simple, os.path.join(models_dir, 'xgb_model_baseline.joblib'))\n",
    "print(\"✅ Modelo XGBoost simple guardado como models/xgb_model_baseline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90067275",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. ¿SE APLICÓ BALANCEO?\n",
    "**Sí.**  \n",
    "Se aplica balanceo de clases **solo si tienes instalada la librería `imblearn`** (SMOTE).  \n",
    "Esto ocurre en este bloque:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote_available = True\n",
    "except ImportError:\n",
    "    smote_available = False\n",
    "\n",
    "# ...\n",
    "if smote_available:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "```\n",
    "**¿Qué hace?**  \n",
    "- Aplica SMOTE (oversampling) **solo al set de entrenamiento** (`X_train`, `y_train`), generando nuevas muestras sintéticas de la clase minoritaria.\n",
    "- El set de test **NO se balancea** (correcto).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ¿CÓMO AFECTA ESO AL VECTORIZADO?\n",
    "- **El vectorizado (TF-IDF) se realiza primero** sobre los textos originales.\n",
    "- Luego, **SMOTE se aplica sobre la matriz TF-IDF** (`X_train`), generando nuevas filas (vectores) sintéticos.\n",
    "- El vectorizador **no se ve afectado**: solo transforma texto a matriz numérica. SMOTE actúa sobre esa matriz.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ¿SE CARGAN LOS CSV Y LOS PKL? ¿QUÉ CSV SE CARGAN, QUÉ PKL SE CARGAN?\n",
    "**Sí, se cargan ambos tipos:**\n",
    "\n",
    "- **CSV:**  \n",
    "  - `train_data.csv` y `test_data.csv` (contienen columnas `text` y `label`).\n",
    "  - Se usan para obtener los textos y las etiquetas.\n",
    "\n",
    "- **PKL:**  \n",
    "  - `tfidf_vectorizer.pkl` (el vectorizador entrenado).\n",
    "  - `X_train_tfidf.pkl` y `X_test_tfidf.pkl` (matrices TF-IDF ya generadas).\n",
    "  - Si no existen los `.pkl` de los datos vectorizados, se crean a partir de los CSV y el vectorizador.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ¿CÓMO ES EL PASO A PASO DEL ENTRENAMIENTO Y USO DE LOS VECTORIZADOS?\n",
    "\n",
    "1. **Carga de datos:**\n",
    "   - Lee los CSV (`train_data.csv`, `test_data.csv`) para obtener textos y etiquetas.\n",
    "2. **Carga o creación de vectorizados:**\n",
    "   - Si existen los `.pkl` de los datos vectorizados (`X_train_tfidf.pkl`, `X_test_tfidf.pkl`), los carga.\n",
    "   - Si no existen, transforma los textos con el vectorizador (`tfidf_vectorizer.pkl`) y guarda los `.pkl`.\n",
    "3. **Balanceo (SMOTE):**\n",
    "   - Si está disponible, aplica SMOTE **solo a `X_train` y `y_train`**.\n",
    "4. **Entrenamiento y evaluación:**\n",
    "   - Usa los datos vectorizados (`X_train`, `y_train`, `X_test`, `y_test`) para entrenar y evaluar los modelos XGBoost.\n",
    "   - El test **nunca se balancea ni se vectoriza de nuevo**.\n",
    "5. **Optimización y selección de modelo:**\n",
    "   - Optuna para hiperparámetros, búsqueda de umbral óptimo, comparación de métricas.\n",
    "6. **Guardado de modelos:**\n",
    "   - El mejor modelo y el baseline se guardan como `.pkl` o `.joblib` en la carpeta models.\n",
    "\n",
    "---\n",
    "\n",
    "**Resumen visual:**\n",
    "\n",
    "```\n",
    "CSV (text, label) ──> TF-IDF vectorizer (.pkl) ──> X_train, X_test (.pkl)\n",
    "                                         │\n",
    "                                         └─> SMOTE (solo X_train) ──> X_train_balanced\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "¿Te gustaría un diagrama o código de ejemplo para visualizar el flujo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b29f14",
   "metadata": {},
   "source": [
    "Aquí tienes el **CUADRO DATOS REALES** con tus métricas reales, siguiendo el formato solicitado:\n",
    "\n",
    "---\n",
    "\n",
    "## CUADRO DATOS REALES ( Graficar distribución de clases: 0 = No tóxico, 1 = Tóxico )\n",
    "\n",
    "### MÉTRICAS ANTES DE OPTIMIZACIÓN\n",
    "\n",
    "| Modelo                 | Accuracy Train | Accuracy Test | F1-score | Recall | Precision | Ajuste      |\n",
    "|------------------------|---------------|--------------|----------|--------|-----------|-------------|\n",
    "| XGBoost Classifier 0   | 0.74          | 0.67         | 0.59     | 0.90   | 0.64      | Overfitting |\n",
    "| XGBoost Classifier 1   | 0.74          | 0.67         | 0.69     | 0.90   | 0.56      | Overfitting |\n",
    "| XGBoost (boosting) 0   | 0.74          | 0.67         | 0.59     | 0.90   | 0.64      | Overfitting |\n",
    "| XGBoost (boosting) 1   | 0.74          | 0.67         | 0.69     | 0.90   | 0.56      | Overfitting |\n",
    "\n",
    "---\n",
    "\n",
    "### MÉTRICAS DESPUÉS DE OPTIMIZACIÓN DE HIPERPARÁMETROS\n",
    "\n",
    "| Modelo                 | Accuracy Train | Accuracy Test | F1-score | Recall | Precision | Ajuste      |\n",
    "|------------------------|---------------|--------------|----------|--------|-----------|-------------|\n",
    "| XGBoost Classifier 0   | 0.82          | 0.71         | 0.66     | 0.90   | 0.77      | Overfitting |\n",
    "| XGBoost Classifier 1   | 0.82          | 0.71         | 0.62     | 0.52   | 0.77      | Overfitting |\n",
    "| XGBoost (boosting) 0   | 0.74          | 0.67         | 0.59     | 0.90   | 0.64      | Overfitting |\n",
    "| XGBoost (boosting) 1   | 0.74          | 0.67         | 0.69     | 0.90   | 0.56      | Overfitting |\n",
    "\n",
    "---\n",
    "\n",
    "### MÉTRICAS LUEGO DE OPTIMIZACIÓN DE UMBRAL\n",
    "\n",
    "| Modelo                 | Accuracy Train | Accuracy Test | F1-score | Recall | Precision | Ajuste      |\n",
    "|------------------------|---------------|--------------|----------|--------|-----------|-------------|\n",
    "| XGBoost Classifier 0   | 0.75          | 0.72         | 0.75     | 0.90   | 0.64      | Buen ajuste |\n",
    "| XGBoost Classifier 1   | 0.75          | 0.72         | 0.69     | 0.90   | 0.56      | Buen ajuste |\n",
    "| XGBoost (boosting) 0   | 0.64          | 0.63         | 0.69     | 0.90   | 0.56      | Buen ajuste |\n",
    "| XGBoost (boosting) 1   | 0.64          | 0.63         | 0.69     | 0.90   | 0.56      | Buen ajuste |\n",
    "\n",
    "---\n",
    "\n",
    "### CUADRO DE RANKING DE MODELOS (XGBoost)\n",
    "\n",
    "| Ranking | Modelo                          | Accuracy Train | Accuracy Test | Precision Test | Recall Test | F1 Test | AUC Test | Diferencia abs | Tipo de ajuste |\n",
    "|---------|---------------------------------|---------------|--------------|---------------|------------|---------|----------|----------------|----------------|\n",
    "| 1       | XGBoost Optuna (umbral óptimo)  | 0.750         | 0.72         | 0.638         | 0.902      | 0.748   | 0.810    | 0.030          | Buen ajuste    |\n",
    "| 2       | XGBoost Optuna                  | 0.820         | 0.71         | 0.774         | 0.522      | 0.623   | 0.810    | 0.110          | Overfitting    |\n",
    "| 3       | XGBoost Base                    | 0.744         | 0.67         | 0.783         | 0.391      | 0.522   | 0.750    | 0.074          | Overfitting    |\n",
    "\n",
    "---\n",
    "\n",
    "✅ El modelo seleccionado es: **XGBoost Classifier (Optuna + umbral óptimo)** con F1-score test = 0.748  \n",
    "Se selecciona el modelo con mayor F1-score en test, porque es la métrica más robusta para clasificación desbalanceada.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿CUÁNDO HACER CROSS-VALIDATION ESTRATIFICADA?\n",
    "\n",
    "- Se recomienda hacer cross-validation estratificada en CADA etapa importante:\n",
    "  - a) Antes de optimizar hiperparámetros: para tener una línea base realista.\n",
    "  - b) Durante la optimización de hiperparámetros: Optuna debe usar cross-validation para evitar overfitting a un solo split.\n",
    "  - c) Después, para validar el modelo final y comparar con test.\n",
    "- Si NO la haces en cada etapa, puedes sobreajustar a un solo split y tus métricas serán poco confiables.\n",
    "- Ventajas: Métricas más robustas, menor riesgo de overfitting, mejor selección de hiperparámetros.\n",
    "- Desventajas: Más lento (más entrenamiento), pero vale la pena para modelos importantes.\n",
    "- Mejor opción: Hacer cross-validation estratificada en cada etapa clave (como en este código).\n",
    "\n",
    "---\n",
    "\n",
    "✅ Mejor modelo guardado como mejor_modelo_xgboost.pkl\n",
    "\n",
    "---\n",
    "\n",
    "**Baseline XGBoost (sin optimización, sin umbral):**  \n",
    "Accuracy: 0.67  \n",
    "Recall: 0.391  \n",
    "Precision: 0.783  \n",
    "F1: 0.522  \n",
    "AUC: 0.750  \n",
    "Matriz de confusión:  \n",
    "[[98 10]  \n",
    " [56 36]]  \n",
    "\n",
    "✅ Modelo XGBoost simple guardado como xgb_model_baseline.joblib\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfec745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.6\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
