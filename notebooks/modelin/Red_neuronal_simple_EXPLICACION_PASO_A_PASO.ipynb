{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732fb3a5",
   "metadata": {},
   "source": [
    "# Proyecto de Machine Learning de Extremo a Extremo - Parte I\n",
    "\n",
    "Este cuaderno es la primera parte de un proyecto completo de Machine Learning.\n",
    "Aquí aprenderemos a detectar comentarios tóxicos usando código y explicaciones sencillas.\n",
    "\n",
    "## ¿Qué son los Proyectos de Machine Learning (ML) de Extremo a Extremo?\n",
    "Imagina que quieres construir un robot que pueda reconocer si una flor es roja o azul.\n",
    "Un proyecto de Machine Learning de extremo a extremo es como hacer todo el trabajo para ese robot, desde enseñarle a mirar las flores hasta ponerlo a trabajar en tu jardín para que las identifique.\n",
    "Cubre cada paso, desde que tienes una idea hasta que el robot está funcionando de verdad.\n",
    "Estos proyectos son muy completos y te enseñan todo lo que necesitas saber sobre cómo hacer que una computadora aprenda.\n",
    "\n",
    "## ¿Por qué son necesarios los Proyectos de ML de Extremo a Extremo?\n",
    "- Aprendes de forma completa: Ves todo el camino, desde que juntas la información (las fotos de las flores) hasta que el robot está listo para usar.\n",
    "- Desarrollas habilidades importantes: Te vuelves bueno resolviendo problemas, usando herramientas especiales y organizando tu trabajo.\n",
    "- Creas soluciones reales: Tu robot puede ayudar en la vida real, ¡identificando flores para ti!\n",
    "- Construyes un portafolio fuerte: Si quieres trabajar con esto, puedes mostrar lo que has hecho.\n",
    "- Ayudas a cumplir objetivos: Si una empresa quiere que un robot clasifique flores, este proyecto asegura que el robot realmente haga lo que la empresa necesita.\n",
    "\n",
    "En esta primera parte, nos enfocaremos en hacer experimentos en este cuaderno, como si fuera nuestro laboratorio.\n",
    "\n",
    "---\n",
    "\n",
    "## PARTE I: Configuración del Cuaderno\n",
    "Empezaremos trayendo todas las herramientas que necesitamos. Piensa en esto como preparar tu caja de herramientas antes de empezar a construir algo.\n",
    "\n",
    "### Importación de Librerías Esenciales\n",
    "Este código nos ayuda a traer las herramientas que vamos a necesitar para nuestro proyecto.\n",
    "\n",
    "**Índice de Pasos en este Código:**\n",
    "1. Importar herramientas para números (NumPy).\n",
    "2. Importar herramientas para tablas de datos (Pandas).\n",
    "3. Importar herramientas para dibujar gráficos (Matplotlib y Seaborn).\n",
    "4. Importar herramientas para trabajar con texto especial (re).\n",
    "5. Importar herramientas para crear nubes de palabras (WordCloud).\n",
    "6. Importar herramientas para construir modelos de aprendizaje (Scikit-learn).\n",
    "7. Importar herramientas para redes neuronales (TensorFlow y Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Esto es como traer una calculadora superpotente al cuaderno. La usaremos para hacer cálculos grandes con números, como sumar muchas cosas a la vez. Es muy útil cuando tenemos muchos datos.\n",
    "import pandas as pd # Esto es como traer una libreta mágica donde podemos organizar información en tablas, como si fueran hojas de cálculo. Es perfecta para guardar y ordenar los comentarios y para saber si son tóxicos o no. La usaremos mucho para ver nuestros datos.\n",
    "from matplotlib import pyplot as plt # Esto es como traer lápices de colores y papel para dibujar. Nos ayudará a hacer dibujos bonitos con nuestros datos, como barras o líneas, para entenderlos mejor. Así podemos ver si hay muchos comentarios tóxicos o pocos.\n",
    " # Este es un truco especial para que los dibujos que hagamos con Matplotlib aparezcan justo aquí, en nuestro cuaderno, sin tener que ir a otro lugar a verlos. Es como dibujar directamente en nuestra hoja de trabajo.\n",
    "import seaborn as sns # Esto es como traer otro juego de lápices y pinceles para dibujar, pero aún más bonitos. Seaborn nos ayuda a hacer gráficos más elaborados y con colores más agradables que Matplotlib, haciendo que nuestros datos se vean mejor.\n",
    "import re # Esto es como traer una lupa mágica para buscar y cambiar letras o palabras en los comentarios. Nos sirve para limpiar el texto y quitar cosas que no necesitamos, como signos de puntuación extraños o errores.\n",
    "from wordcloud import WordCloud # Esto es como traer un juguete que hace nubes con palabras. Con esto, podemos ver las palabras más grandes y populares en los comentarios, como si fueran nubes formadas por palabras. Las palabras más usadas aparecerán más grandes.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Esta es una herramienta de un kit de construcción (Scikit-learn) que nos ayuda a convertir las palabras en números. Imagina que cada palabra tiene un número secreto y esta herramienta lo descubre para que la computadora pueda entenderlas y aprender de ellas.\n",
    "from sklearn.model_selection import train_test_split # Esta es otra herramienta del kit que nos ayuda a dividir nuestros datos. Es como si tuviéramos un montón de juguetes y los separamos en dos grupos: uno para que nuestro modelo practique (entrenamiento) y otro para ver si lo que aprendió funciona bien con juguetes que nunca ha visto (prueba).\n",
    "from sklearn.metrics import classification_report # Esta herramienta del kit nos ayuda a saber qué tan bien le fue a nuestro modelo. Es como un boletín de calificaciones para nuestro modelo, diciéndonos si hizo un buen trabajo o si necesita mejorar al clasificar los comentarios.\n",
    "import tensorflow as tf # Esto es como traer un cerebro gigante al cuaderno. TensorFlow es una herramienta muy avanzada que nos permite construir \"cerebritos\" artificiales, llamados redes neuronales, que aprenden a reconocer patrones, como si un comentario es tóxico. Es el motor principal de nuestro modelo.\n",
    "from tensorflow.keras.models import Sequential # Esto es como elegir un tipo de caja para construir nuestro cerebrito artificial. \"Sequential\" significa que el cerebrito va a aprender paso a paso, como una cadena donde la información fluye de una capa a la siguiente.\n",
    "from tensorflow.keras.layers import Dense, Dropout # Estas son las piezas que ponemos dentro de la caja de nuestro cerebrito.\n",
    "# 'Dense' es como una capa de neuronas (pequeñas celdas que aprenden) donde cada neurona está conectada con todas las neuronas de la capa anterior. Es como si todas las ideas de un paso se compartieran con el siguiente, ayudando al modelo a entender relaciones complejas.\n",
    "# 'Dropout' es una pieza que ayuda a que el cerebrito no \"memorice\" demasiado. Imagina que a veces, algunas neuronas se toman un descanso (se apagan un rato), para que el cerebrito aprenda de muchas maneras diferentes y no se vuelva flojo. Esto previene que el modelo se \"sobreajuste\" a los datos de entrenamiento.\n",
    "from tensorflow.keras.optimizers import Adam # Esto es como el entrenador de nuestro cerebrito. Adam es un tipo especial de entrenador que le enseña al cerebrito cómo aprender más rápido y mejor, ajustando las conexiones entre las neuronas para que cometa menos errores y mejore su precisión con el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ec74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72be21",
   "metadata": {},
   "source": [
    "### Herramientas y su función\n",
    "\n",
    "| Herramienta | Función Principal | ¿Para qué sirve? |\n",
    "|-------------|------------------|------------------|\n",
    "| numpy | Cálculos numéricos y arreglos | Realizar operaciones matemáticas eficientes con grandes conjuntos de datos, como sumar miles de números. |\n",
    "| pandas | Manipulación y análisis de datos | Organizar datos en tablas (como hojas de cálculo) para facilitar su lectura, limpieza y análisis. |\n",
    "| matplotlib.pyplot | Visualización de datos básica | Crear gráficos simples como barras o líneas para ver cómo se distribuyen los datos. |\n",
    "| %matplotlib inline | Integración de gráficos en Jupyter | Hace que los gráficos que dibujamos aparezcan justo aquí, en nuestro cuaderno. |\n",
    "| seaborn | Visualización de datos avanzada | Crear gráficos estadísticos más complejos y bonitos para explorar relaciones entre variables. |\n",
    "| re | Expresiones regulares (texto) | Buscar, reemplazar y manipular patrones de texto de forma avanzada, útil para limpiar comentarios. |\n",
    "| WordCloud | Visualización de nubes de palabras | Generar imágenes donde las palabras más usadas aparecen más grandes. |\n",
    "| TfidfVectorizer | Vectorización de texto (TF-IDF) | Convertir las palabras de los comentarios en números (vectores) que los modelos de ML pueden entender. |\n",
    "| train_test_split | División de datos | Separar nuestros datos en dos grupos: uno para que el modelo aprenda y otro para ver si aprendió bien. |\n",
    "| classification_report | Evaluación del modelo | Generar un informe detallado que nos dice qué tan bien el modelo clasifica los comentarios. |\n",
    "| tensorflow | Plataforma de ML/DL | Proporcionar las herramientas para construir y entrenar redes neuronales profundas. |\n",
    "| Sequential | Tipo de modelo Keras | Definir nuestro \"cerebro\" artificial (red neuronal) de una manera sencilla. |\n",
    "| Dense | Capa de red neuronal | Son las \"neuronas\" de nuestro \"cerebro\". Cada una aprende algo y pasa su conocimiento a la siguiente capa. |\n",
    "| Dropout | Regularización de red neuronal | Desactivar aleatoriamente algunas \"neuronas\" durante el entrenamiento para que el \"cerebro\" no se vuelva perezoso. |\n",
    "| Adam | Optimizador de red neuronal | Es el \"entrenador\" de nuestro \"cerebro\". Le enseña cómo ajustar sus conexiones para cometer menos errores. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2ad59",
   "metadata": {},
   "source": [
    "### Cuento para niños: Las Herramientas del Detective de Palabras Mágicas\n",
    "\n",
    "En un pueblo muy especial, la gente se comunicaba con palabras que aparecían en una pantalla mágica. La mayoría de las palabras eran brillantes y amables, pero a veces, aparecían palabras con pequeñas espinas, que hacían que los demás se sintieran tristes o molestos. Esas eran las \"palabras tóxicas\".\n",
    "\n",
    "Un día, el Detective de Palabras Mágicas decidió que quería ayudar a mantener el pueblo feliz. Para eso, necesitaba construir un asistente muy inteligente que pudiera encontrar las palabras tóxicas. Abrió su gran caja de herramientas, ¡y esto es lo que encontró!\n",
    "\n",
    "(Lee el cuento completo en la celda original para más detalles sobre cada herramienta y su función.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d950d",
   "metadata": {},
   "source": [
    "### Añadiendo Conjuntos de Datos\n",
    "\n",
    "Usaremos dos colecciones de comentarios para este proyecto:\n",
    "1. Desafío de Clasificación de Comentarios Tóxicos (¡una colección grande!)\n",
    "2. Comentarios tóxicos de Youtube (¡una colección más pequeña pero también útil!)\n",
    "\n",
    "Vamos a añadir estos datos a nuestro cuaderno. Imagina que son dos cajas llenas de notas con comentarios.\n",
    "\n",
    "*(Aquí iría la imagen que mencionas en el texto original, pero como no puedo generar imágenes, se omite.)*\n",
    "\n",
    "Ahora, vamos a importar (traer) estos conjuntos de datos a nuestras tablas de `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Título del Bloque de Código: Cargar y Combinar Conjuntos de Datos de Comentarios\n",
    "\n",
    "# Índice de Pasos en este Código:\n",
    "# 1. Cargar el primer conjunto de datos (comentarios de Kaggle).\n",
    "# 2. Cargar el segundo conjunto de datos (comentarios de YouTube).\n",
    "# 3. Procesar el primer conjunto de datos para que tenga solo el texto y si es tóxico.\n",
    "# 4. Procesar el segundo conjunto de datos para que tenga solo el texto y si es tóxico.\n",
    "# 5. Unir los dos conjuntos de datos procesados en una sola tabla grande.\n",
    "# 6. Mostrar las primeras filas de la tabla combinada para ver cómo quedó.\n",
    "\n",
    "# Paso 1: Cargar el primer conjunto de datos (comentarios de Kaggle)\n",
    "df1 = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
    "\n",
    "# Paso 2: Cargar el segundo conjunto de datos (comentarios de YouTube)\n",
    "df2 = pd.read_csv('/kaggle/input/youtube-toxicity-data/youtoxic_english_1000.csv')\n",
    "\n",
    "# Paso 3: Procesar df1 (el primer conjunto de datos)\n",
    "df1['Toxic'] = df1.iloc[:, 2:].any(axis=1)\n",
    "df1_processed = df1[['comment_text', 'Toxic']].rename(columns={'comment_text': 'Text'})\n",
    "\n",
    "# Paso 4: Procesar df2 (el segundo conjunto de datos)\n",
    "df2['Toxic'] = df2.iloc[:, 3:].any(axis=1)\n",
    "df2_processed = df2[['Text', 'Toxic']]\n",
    "\n",
    "# Paso 5: Combinar df1_processed y df2_processed\n",
    "df = pd.concat([df1_processed, df2_processed], ignore_index=True)\n",
    "\n",
    "# Paso 6: Mostrar las primeras filas de final_df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583cbea",
   "metadata": {},
   "source": [
    "### Herramientas clave en la carga y combinación de datos\n",
    "\n",
    "| Herramienta | Función Principal | ¿Para qué sirve? |\n",
    "|-------------|------------------|------------------|\n",
    "| pd.read_csv() | Cargar datos de archivos CSV | Traer la información de los comentarios que tenemos guardados en archivos a nuestro cuaderno para poder trabajar con ellos. |\n",
    "| df.iloc[:, 2:].any(axis=1) | Simplificar etiquetas de toxicidad | Unificar varias categorías de toxicidad en una sola etiqueta \"Tóxico\" (Verdadero/Falso). |\n",
    "| df[['comment_text', 'Toxic']] | Seleccionar columnas específicas | Quedarnos solo con las columnas importantes que necesitamos: el texto del comentario y su nueva etiqueta de toxicidad. |\n",
    "| .rename(columns={'comment_text': 'Text'}) | Renombrar columnas | Cambiar el nombre de una columna para que sea más fácil de entender o para que coincida con otros datos. |\n",
    "| pd.concat([...], ignore_index=True) | Combinar tablas de datos | Unir dos o más tablas de comentarios en una sola tabla grande para tener toda la información junta. |\n",
    "| df.head() | Visualizar las primeras filas | Mostrar las primeras filas de la tabla combinada para comprobar que la unión se hizo correctamente. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611c49f",
   "metadata": {},
   "source": [
    "### Cuento para niños: Uniendo las Cajas de Comentarios\n",
    "\n",
    "Imagina que nuestro Detective de Palabras Mágicas tiene dos cajas llenas de notas. Una caja es muy grande y tiene Notas de Desafío (el primer dataset), y la otra es más pequeña y tiene Notas de YouTube (el segundo dataset). Dentro de cada nota, hay un comentario.\n",
    "\n",
    "El Detective quería juntar todas las notas en una sola caja gigante para que su asistente inteligente aprendiera de todas a la vez.\n",
    "\n",
    "Primero, el Detective usó su \"Cargador de Cajas\" (pd.read_csv). Con él, abrió la Caja de Desafío y la Caja de YouTube, y las puso sobre la mesa, listas para trabajar.\n",
    "\n",
    "Pero las notas de la Caja de Desafío eran un poco complicadas. Algunas decían \"esta palabra es fea\", otras \"esta palabra es grosera\", y otras \"esta palabra amenaza\". El Detective pensó: \"¡Uhm, demasiadas etiquetas! Mejor que solo diga si es 'espinosa' o no\". Así que usó su \"Simplificador de Etiquetas\" (.iloc[:, 2:].any(axis=1)). Este simplificador miraba todas esas etiquetas y, si alguna era \"sí\", ponía un gran \"SÍ, ES ESPINOSA\" en la nota. Si todas eran \"no\", entonces ponía \"NO ES ESPINOSA\".\n",
    "\n",
    "Además, las notas de la Caja de Desafío tenían una parte que decía \"Texto del Comentario\", y las de la Caja de YouTube solo decían \"Texto\". Para que todo fuera igual, el Detective usó su \"Renombrador Mágico\" (.rename(columns={'comment_text': 'Text'})) para que en todas las notas, la parte del comentario se llamara simplemente \"Texto\". ¡Así era más ordenado!\n",
    "\n",
    "Una vez que todas las notas estaban simplificadas y con nombres iguales, el Detective usó su \"Pegamento de Cajas\" (pd.concat). Con este pegamento especial, ¡unió todas las notas de la Caja de Desafío y la Caja de YouTube en una sola Caja Gigante de Comentarios! Asegurándose de que cada nota tuviera un número único para que no se mezclaran.\n",
    "\n",
    "Finalmente, para ver si todo había quedado bien, el Detective miró las primeras cinco notas de la Caja Gigante (df.head()). ¡Y sí! Todo estaba perfecto, con cada comentario y su etiqueta de \"espinoso\" o \"no espinoso\". Ahora, el asistente inteligente tenía una gran colección de notas para empezar a aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22eec1d",
   "metadata": {},
   "source": [
    "### Explorando y Limpiando los Datos\n",
    "\n",
    "Ahora que tenemos nuestro conjunto de datos combinado, vamos a hacer lo que a la mayoría de los científicos de datos les gusta hacer con los datos: ¡jugar con ellos!\n",
    "\n",
    "A continuación, realizaremos un resumen estadístico, revisaremos tipos de datos, eliminaremos duplicados y veremos la distribución de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877eb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Título del Bloque de Código: Exploración y Limpieza Inicial de Datos\n",
    "\n",
    "# 1. Obtener un resumen estadístico de los datos\n",
    "print(df.describe())\n",
    "\n",
    "# 2. Verificar tipos de datos y valores faltantes\n",
    "print(df.dtypes)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Verificar comentarios duplicados\n",
    "print(\"Duplicate rows based on 'Text' column:\")\n",
    "duplicate_rows = df[df.duplicated(subset=['Text'], keep=False)]\n",
    "print(duplicate_rows)\n",
    "\n",
    "# 4. Eliminar comentarios duplicados\n",
    "df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
    "\n",
    "# 5. Confirmar que los duplicados fueron eliminados y reindexar\n",
    "print(\"Number of rows after removing duplicates:\", len(df))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 6. Distribución de la columna 'Toxic'\n",
    "toxic_distribution = df['Toxic'].value_counts()\n",
    "print(toxic_distribution)\n",
    "\n",
    "# 7. Guardar nuestra tabla limpia en un nuevo archivo CSV\n",
    "df.to_csv(\"Toxic_Comments_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee874a7",
   "metadata": {},
   "source": [
    "### Herramientas clave en la limpieza y exploración de datos\n",
    "\n",
    "| Herramienta | Función Principal | ¿Para qué sirve? |\n",
    "|-------------|------------------|------------------|\n",
    "| df.describe() | Resumen estadístico | Obtener rápidamente información sobre la cantidad de datos, promedios, etc. |\n",
    "| df.dtypes | Comprobar tipos de datos | Verificar si las columnas tienen el tipo de información correcto. |\n",
    "| df.isnull().sum() | Contar valores nulos | Saber cuántas \"casillas vacías\" o datos faltantes hay en cada columna. |\n",
    "| df.duplicated() y drop_duplicates() | Identificar y eliminar duplicados | Encontrar comentarios que están repetidos y borrarlos. |\n",
    "| df.reset_index() | Reindexar el DataFrame | Volver a numerar las filas de nuestra tabla de datos de forma consecutiva. |\n",
    "| df['Toxic'].value_counts() | Contar la distribución de clases | Saber cuántos comentarios son tóxicos y cuántos no lo son. |\n",
    "| df.to_csv() | Guardar datos en archivo CSV | Guardar la tabla de datos ya limpia y procesada en un nuevo archivo. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08b2d0",
   "metadata": {},
   "source": [
    "### Limpiando el Texto de los Comentarios\n",
    "Ahora que tenemos los datos limpios y sin duplicados, es hora de limpiar el texto de los comentarios. Esto significa quitar cosas que no ayudan a nuestro modelo a aprender, como signos de puntuación, números, o palabras muy comunes que no aportan mucho significado (como \"el\", \"la\", \"de\").\n",
    "También convertiremos todo a minúsculas para que la computadora no piense que \"Hola\" y \"hola\" son palabras diferentes.\n",
    "\n",
    "#### ¿Por qué limpiar el texto?\n",
    "Imagina que tu asistente inteligente está aprendiendo a leer. Si le das libros llenos de garabatos, errores y palabras repetidas, ¡le costará mucho aprender! Limpiar el texto es como darle libros bien escritos y ordenados.\n",
    "\n",
    "A continuación, crearemos una función para limpiar el texto y la aplicaremos a todos los comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34afe42",
   "metadata": {},
   "source": [
    "### Limpieza de Texto: ¿Por qué es importante?\n",
    "\n",
    "Los comentarios pueden tener errores, símbolos raros, mayúsculas, emojis, URLs, menciones y muchas cosas que no ayudan a nuestro modelo a aprender. Limpiar el texto es como preparar los ingredientes antes de cocinar: si quitamos lo que no sirve, el platillo (nuestro modelo) saldrá mucho mejor.\n",
    "\n",
    "**¿Qué vamos a limpiar?**\n",
    "- Convertir todo a minúsculas (para que 'Hola' y 'hola' sean lo mismo).\n",
    "- Quitar URLs (enlaces a páginas web).\n",
    "- Quitar menciones (@usuario).\n",
    "- Quitar signos de puntuación y caracteres especiales.\n",
    "- Quitar números.\n",
    "- Quitar espacios extra.\n",
    "- (Opcional) Quitar palabras vacías (stopwords) y lematizar (convertir palabras a su forma base).\n",
    "\n",
    "¡Vamos a crear una función mágica para limpiar los comentarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeffaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargamos recursos necesarios de NLTK (solo la primera vez)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Creamos la función de limpieza de texto\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    Función para limpiar comentarios de texto.\n",
    "    - Convierte a minúsculas\n",
    "    - Elimina URLs, menciones, signos de puntuación, números y espacios extra\n",
    "    - Elimina stopwords y lematiza las palabras\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Eliminar URLs\n",
    "    texto = re.sub(r'http\\S+|www\\S+', '', texto)\n",
    "    # Eliminar menciones (@usuario)\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Eliminar signos de puntuación\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Eliminar números\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "    # Eliminar caracteres especiales y tildes\n",
    "    texto = ''.join(c for c in unicodedata.normalize('NFD', texto)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "    # Eliminar espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    # Eliminar stopwords y lematizar\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    palabras = texto.split()\n",
    "    palabras = [lemmatizer.lemmatize(palabra) for palabra in palabras if palabra not in stop_words]\n",
    "    texto_limpio = ' '.join(palabras)\n",
    "    return texto_limpio\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ejemplo = \"@usuario ¡Hola! Visita https://www.ejemplo.com para más info. #NLP 2023\"\n",
    "print(\"Original:\", ejemplo)\n",
    "print(\"Limpio:\", limpiar_texto(ejemplo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdf40e",
   "metadata": {},
   "source": [
    "### Aplicando la función de limpieza a todos los comentarios\n",
    "\n",
    "Ahora que tenemos nuestra función mágica, la aplicaremos a todos los comentarios del dataset. Esto puede tardar un poco si hay muchos datos, ¡pero el resultado será un texto mucho más limpio y fácil de analizar!\n",
    "\n",
    "**¿Qué logramos con esto?**\n",
    "- Reducimos el ruido en los datos.\n",
    "- Hacemos que el modelo aprenda mejor.\n",
    "- Preparamos el texto para la siguiente etapa: vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que el DataFrame combinado se llama 'df' y la columna de texto es 'comment_text'\n",
    "# Si tu columna tiene otro nombre, cámbialo aquí\n",
    "\n",
    "df['comment_text_limpio'] = df['comment_text'].apply(limpiar_texto)\n",
    "\n",
    "# Veamos algunos ejemplos antes y después\n",
    "df[['comment_text', 'comment_text_limpio']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad0722",
   "metadata": {},
   "source": [
    "| Función / Herramienta         | ¿Para qué sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `re.sub`                     | Buscar y reemplazar patrones en texto (regex)                  | Quitar URLs, menciones          |\n",
    "| `str.lower()`                | Convertir texto a minúsculas                                   | 'Hola' → 'hola'                 |\n",
    "| `str.translate`              | Eliminar signos de puntuación                                  | '¡Hola!' → 'Hola'               |\n",
    "| `unicodedata.normalize`      | Quitar tildes y caracteres especiales                          | 'acción' → 'accion'             |\n",
    "| `stopwords.words`            | Lista de palabras vacías (stopwords)                           | 'the', 'and', 'is', ...         |\n",
    "| `WordNetLemmatizer`          | Lematizar palabras (forma base)                                | 'running' → 'run'               |\n",
    "| `apply` de pandas            | Aplicar una función a cada elemento de una columna             | Limpiar todos los comentarios    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269f76e",
   "metadata": {},
   "source": [
    "#### Cuento: \"La escoba mágica de los comentarios\"\n",
    "\n",
    "Había una vez un reino donde los comentarios eran muy desordenados. Algunos tenían palabras raras, otros estaban llenos de símbolos y algunos hablaban en mayúsculas todo el tiempo. Los sabios del reino querían entender qué decían los comentarios, pero no podían porque había mucho ruido.\n",
    "\n",
    "Un día, llegó una escoba mágica llamada `limpiar_texto`. Esta escoba barría todo lo que no servía: URLs, menciones, signos de puntuación, números y hasta palabras que no decían nada importante. Cuando los sabios usaron la escoba mágica, los comentarios quedaron limpios y ordenados, listos para ser analizados por el gran oráculo (el modelo de machine learning).\n",
    "\n",
    "Desde entonces, cada vez que alguien quería entender los comentarios, primero usaba la escoba mágica. Y así, el reino pudo descubrir cuáles comentarios eran tóxicos y cuáles no, ¡y todos vivieron más felices y menos ofendidos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff97637",
   "metadata": {},
   "source": [
    "### Vectorización de Texto: ¿Cómo convertimos palabras en números?\n",
    "\n",
    "Las computadoras no entienden palabras, ¡entienden números! Para que nuestro modelo pueda aprender de los comentarios, necesitamos transformar el texto en una matriz de números.\n",
    "\n",
    "**¿Cómo lo hacemos?**\n",
    "Usaremos una técnica llamada **TF-IDF (Term Frequency - Inverse Document Frequency)**. Esta técnica nos ayuda a saber qué palabras son importantes en cada comentario, dándoles un peso especial según cuántas veces aparecen y si son raras o comunes en el conjunto de datos.\n",
    "\n",
    "**Ventajas de TF-IDF:**\n",
    "- Da más importancia a palabras únicas de cada comentario.\n",
    "- Reduce el peso de palabras muy comunes.\n",
    "- Es fácil de usar y funciona bien para muchos problemas de texto.\n",
    "\n",
    "¡Vamos a vectorizar nuestros comentarios limpios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creamos el vectorizador TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Puedes ajustar el número de características\n",
    "\n",
    "# Ajustamos y transformamos los comentarios limpios\n",
    "tfidf_matrix = tfidf.fit_transform(df['comment_text_limpio'])\n",
    "\n",
    "# Convertimos la matriz a un DataFrame para verla mejor\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126db74",
   "metadata": {},
   "source": [
    "| Función / Herramienta         | ¿Para qué sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `TfidfVectorizer`            | Convierte texto en una matriz de pesos TF-IDF                  | Vectorizar comentarios          |\n",
    "| `fit_transform`              | Ajusta el vectorizador y transforma el texto                   | Crear matriz numérica           |\n",
    "| `get_feature_names_out`      | Obtiene los nombres de las palabras (features)                 | Ver columnas del DataFrame      |\n",
    "| `pd.DataFrame`               | Convierte la matriz a un DataFrame de pandas                   | Visualizar la matriz            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e16a9",
   "metadata": {},
   "source": [
    "#### Cuento: \"El traductor de palabras mágicas\"\n",
    "\n",
    "En el reino de los comentarios limpios, los sabios querían que el oráculo (la computadora) entendiera los mensajes. Pero el oráculo solo hablaba en números, no en palabras.\n",
    "\n",
    "Entonces, inventaron un traductor mágico llamado **TF-IDF**. Este traductor tomaba cada palabra y le daba un número especial: si la palabra era muy común, recibía un número pequeño; si era rara y especial, recibía un número grande.\n",
    "\n",
    "Así, cada comentario se transformó en una fila de números, y el oráculo pudo empezar a aprender y a predecir cuáles comentarios eran tóxicos y cuáles no. ¡El reino estaba cada vez más cerca de resolver el misterio de la toxicidad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d635d6",
   "metadata": {},
   "source": [
    "# ADVERTENCIA DE ALGO QUE NO SE DEBE HACER \n",
    "\n",
    "No es lo ideal vectorizar y tokenizar **antes** de dividir en train y test. Te explico por qué:\n",
    "\n",
    "### ¿Por qué es un error vectorizar antes de dividir?\n",
    "Cuando aplicas `TfidfVectorizer` (o cualquier vectorizador/tokenizador) **antes** de dividir los datos, el vectorizador \"ve\" todo el texto, incluyendo los datos de test. Esto significa que:\n",
    "- El vocabulario y los pesos TF-IDF se calculan usando información de los datos de test.\n",
    "- El modelo puede aprender palabras o patrones que solo existen en el test, lo que genera **fugas de información** (data leakage).\n",
    "- Las métricas de evaluación serán demasiado optimistas y no reflejarán el rendimiento real en datos nuevos.\n",
    "\n",
    "### ¿Cuál es la forma correcta?\n",
    "1. **Divide** primero el dataset en train y test (`train_test_split`).\n",
    "2. **Ajusta** (`fit`) el vectorizador **solo** con los datos de entrenamiento.\n",
    "3. **Transforma** (`transform`) tanto el train como el test usando ese vectorizador ya ajustado.\n",
    "\n",
    "#### Ejemplo correcto:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Divide primero\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Toxic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Ajusta solo con train\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# 3. Transforma test con el mismo vectorizador\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "```\n",
    "\n",
    "### Resumen\n",
    "- **No** vectorices/tokenices antes de dividir.\n",
    "- **Sí** divide primero, luego ajusta el vectorizador solo con train, y después transforma ambos conjuntos.\n",
    "\n",
    "Esto evita fugas de información y asegura una evaluación justa y realista de tu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9c47",
   "metadata": {},
   "source": [
    "### Balanceo de Clases: ¿Qué pasa si hay pocos comentarios tóxicos?\n",
    "\n",
    "En muchos problemas de clasificación, como la detección de comentarios tóxicos, suele haber muchos más ejemplos de una clase que de otra (por ejemplo, muchos comentarios no tóxicos y pocos tóxicos). Esto puede hacer que el modelo aprenda a ignorar la clase minoritaria.\n",
    "\n",
    "**¿Cómo lo solucionamos?**\n",
    "Usaremos una técnica llamada **SMOTE (Synthetic Minority Over-sampling Technique)**, que crea ejemplos sintéticos de la clase minoritaria para balancear el dataset.\n",
    "\n",
    "**Ventajas de SMOTE:**\n",
    "- Ayuda a que el modelo no se sesgue hacia la clase mayoritaria.\n",
    "- Mejora la capacidad de detectar comentarios tóxicos.\n",
    "- Es fácil de aplicar después de vectorizar los datos.\n",
    "\n",
    "¡Vamos a balancear nuestras clases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Definimos las variables X (características) e y (etiqueta)\n",
    "X = tfidf_df\n",
    "# Suponiendo que la columna de la etiqueta es 'toxic' (ajusta si tu columna tiene otro nombre)\n",
    "y = df['toxic']\n",
    "\n",
    "# Creamos el objeto SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Aplicamos SMOTE para balancear las clases\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Veamos la nueva distribución de clases\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(x=y_res)\n",
    "plt.title('Distribución de clases después de SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88235ee",
   "metadata": {},
   "source": [
    "| Función / Herramienta         | ¿Para qué sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `SMOTE`                      | Genera ejemplos sintéticos de la clase minoritaria             | Balancear clases                |\n",
    "| `fit_resample`               | Ajusta y aplica el balanceo a los datos                        | Obtener X_res, y_res            |\n",
    "| `sns.countplot`              | Grafica la distribución de clases                              | Visualizar balanceo             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9fd90",
   "metadata": {},
   "source": [
    "#### Cuento: \"El mago que clonaba comentarios\"\n",
    "\n",
    "En el reino de los comentarios, había un problema: los comentarios tóxicos eran muy pocos y los no tóxicos eran muchísimos. El oráculo (modelo) solo aprendía de los no tóxicos y se olvidaba de los tóxicos.\n",
    "\n",
    "Un día, llegó un mago llamado **SMOTE**. Este mago tenía el poder de crear clones mágicos de los comentarios tóxicos, para que hubiera la misma cantidad que de los no tóxicos. Así, el oráculo pudo aprender de ambos por igual y se volvió mucho más justo y sabio.\n",
    "\n",
    "Desde entonces, cada vez que había un desequilibrio, llamaban al mago SMOTE para que ayudara a balancear el reino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcdc7a",
   "metadata": {},
   "source": [
    "### Construcción y Entrenamiento del Modelo: ¡Hora de predecir!\n",
    "\n",
    "Ahora que tenemos nuestros datos limpios, vectorizados y balanceados, ¡es momento de construir nuestro modelo! Usaremos una **red neuronal simple** para predecir si un comentario es tóxico o no.\n",
    "\n",
    "**¿Por qué una red neuronal?**\n",
    "- Puede aprender patrones complejos en los datos.\n",
    "- Es flexible y se adapta a diferentes problemas.\n",
    "- Aunque es simple, puede lograr buenos resultados en tareas de texto.\n",
    "\n",
    "¡Vamos a construir y entrenar nuestro modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4334ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definimos la arquitectura de la red neuronal\n",
    "modelo = Sequential()\n",
    "modelo.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "modelo.add(Dropout(0.5))\n",
    "modelo.add(Dense(64, activation='relu'))\n",
    "modelo.add(Dropout(0.3))\n",
    "modelo.add(Dense(1, activation='sigmoid'))  # Salida binaria\n",
    "\n",
    "# Compilamos el modelo\n",
    "modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Definimos early stopping para evitar sobreajuste\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "historial = modelo.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80294a",
   "metadata": {},
   "source": [
    "| Función / Herramienta         | ¿Para qué sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `Sequential`                 | Crear un modelo de red neuronal secuencial                     | Definir la arquitectura         |\n",
    "| `Dense`                      | Capa densa (totalmente conectada)                              | Añadir capas al modelo          |\n",
    "| `Dropout`                    | Evitar sobreajuste eliminando neuronas aleatoriamente          | Regularización                  |\n",
    "| `compile`                    | Configurar el modelo (optimizador, función de pérdida, métrica)| Preparar para entrenar          |\n",
    "| `fit`                        | Entrenar el modelo con los datos                               | Ajustar pesos                   |\n",
    "| `EarlyStopping`              | Detener el entrenamiento si no mejora                          | Evitar sobreajuste              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855f6e2",
   "metadata": {},
   "source": [
    "#### Cuento: \"El aprendiz que aprendía de ejemplos\"\n",
    "\n",
    "En el reino, los sabios construyeron un aprendiz especial llamado **Red Neuronal**. Este aprendiz tenía muchas neuronas (pequeños ayudantes) que se pasaban mensajes entre sí para aprender a distinguir comentarios tóxicos de los que no lo eran.\n",
    "\n",
    "Cada vez que el aprendiz veía un ejemplo, ajustaba sus conexiones para mejorar. Si se equivocaba, aprendía del error y lo intentaba de nuevo. Así, poco a poco, se volvió muy bueno prediciendo la toxicidad de los comentarios.\n",
    "\n",
    "Y así, el reino pudo confiar en su aprendiz para mantener la paz y la armonía en los comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673c49c",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo: ¿Qué tan bien predice?\n",
    "\n",
    "Después de entrenar nuestro modelo, es importante saber qué tan bien funciona. Para eso, evaluaremos su desempeño usando el conjunto de prueba y diferentes métricas.\n",
    "\n",
    "**¿Qué vamos a ver?**\n",
    "- Precisión (accuracy)\n",
    "- Matriz de confusión\n",
    "- Curva ROC y AUC\n",
    "- Gráficas de la historia de entrenamiento\n",
    "\n",
    "¡Vamos a evaluar a nuestro aprendiz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ded245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_prob = modelo.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Precisión\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión (accuracy): {acc:.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "cmp = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusión:\\n\", cmp)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Curva ROC y AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Gráficas de la historia de entrenamiento\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(historial.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(historial.history['val_accuracy'], label='Validación')\n",
    "plt.title('Precisión durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(historial.history['loss'], label='Entrenamiento')\n",
    "plt.plot(historial.history['val_loss'], label='Validación')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89157b3",
   "metadata": {},
   "source": [
    "| Función / Herramienta         | ¿Para qué sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `accuracy_score`             | Calcula la precisión del modelo                                | Medir desempeño                 |\n",
    "| `confusion_matrix`           | Muestra aciertos y errores por clase                           | Ver errores del modelo          |\n",
    "| `classification_report`      | Resumen de métricas (precision, recall, f1)                    | Evaluar a fondo                 |\n",
    "| `roc_curve`, `auc`           | Calcular y graficar la curva ROC y el área bajo la curva       | Medir discriminación            |\n",
    "| `plt.plot`                   | Graficar resultados y evolución del entrenamiento              | Visualizar desempeño            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffea7dd",
   "metadata": {},
   "source": [
    "#### Cuento: \"El examen del aprendiz\"\n",
    "\n",
    "Después de mucho entrenamiento, el aprendiz (red neuronal) tuvo que presentar un examen. Los sabios le dieron comentarios que nunca había visto y observaron cómo respondía.\n",
    "\n",
    "Algunas veces acertó, otras se equivocó, pero cada resultado fue anotado en una gran pizarra (la matriz de confusión). También midieron qué tan bien distinguía entre tóxicos y no tóxicos usando una cuerda mágica llamada **curva ROC**.\n",
    "\n",
    "Gracias a este examen, los sabios supieron si el aprendiz estaba listo para ayudar en el reino o si necesitaba más práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07352fe9",
   "metadata": {},
   "source": [
    "### Guardando el Modelo y el Vectorizador: ¡Para usarlo después!\n",
    "\n",
    "Una vez que tenemos un modelo entrenado y un vectorizador TF-IDF, es muy útil guardarlos para poder usarlos en el futuro sin tener que volver a entrenar todo desde cero.\n",
    "\n",
    "**¿Qué vamos a guardar?**\n",
    "- El modelo de red neuronal (en formato HDF5 o SavedModel de Keras).\n",
    "- El vectorizador TF-IDF (con pickle).\n",
    "\n",
    "¡Vamos a guardar nuestro trabajo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el modelo de Keras\n",
    "modelo.save('../models/modelo_red_neuronal.h5')\n",
    "\n",
    "# Guardar el vectorizador TF-IDF\n",
    "with open('../data/processed/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"Modelo y vectorizador guardados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a1ee3",
   "metadata": {},
   "source": [
    "### Usando el Modelo Guardado: ¡Predice nuevos comentarios!\n",
    "\n",
    "Ahora que tenemos nuestro modelo y vectorizador guardados, podemos cargarlos en cualquier momento y usarlos para predecir si un nuevo comentario es tóxico o no.\n",
    "\n",
    "**¿Cómo lo hacemos?**\n",
    "- Cargamos el modelo y el vectorizador.\n",
    "- Limpiamos el nuevo comentario.\n",
    "- Lo transformamos con el vectorizador.\n",
    "- Usamos el modelo para predecir.\n",
    "\n",
    "¡Vamos a probarlo con un ejemplo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21859b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Cargar el modelo y el vectorizador\n",
    "tfidf_loaded = None\n",
    "with open('../data/processed/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_loaded = pickle.load(f)\n",
    "modelo_loaded = load_model('../models/modelo_red_neuronal.h5')\n",
    "\n",
    "# Nuevo comentario para predecir\n",
    "nuevo_comentario = \"You are so stupid and ugly!\"\n",
    "\n",
    "# Limpiar el comentario\n",
    "comentario_limpio = limpiar_texto(nuevo_comentario)\n",
    "\n",
    "# Vectorizar\n",
    "comentario_vectorizado = tfidf_loaded.transform([comentario_limpio])\n",
    "\n",
    "# Predecir\n",
    "prediccion = modelo_loaded.predict(comentario_vectorizado)\n",
    "if prediccion[0][0] > 0.5:\n",
    "    print(\"El comentario es TÓXICO\")\n",
    "else:\n",
    "    print(\"El comentario NO es tóxico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be6d3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ¡Felicidades! Has completado un proyecto de Machine Learning de Extremo a Extremo\n",
    "\n",
    "En este cuaderno aprendiste a:\n",
    "- Cargar y combinar datos de diferentes fuentes.\n",
    "- Limpiar y preparar texto para análisis.\n",
    "- Vectorizar texto con TF-IDF.\n",
    "- Balancear clases con SMOTE.\n",
    "- Construir, entrenar y evaluar una red neuronal simple.\n",
    "- Guardar y reutilizar tu modelo y vectorizador.\n",
    "- Predecir la toxicidad de nuevos comentarios.\n",
    "\n",
    "### Recuerda:\n",
    "Cada paso es importante y, aunque aquí usamos ejemplos sencillos y cuentos, ¡estos conceptos se aplican en proyectos reales!\n",
    "\n",
    "Sigue practicando, experimenta con otros modelos y datasets, y conviértete en un gran detective de palabras mágicas.\n",
    "\n",
    "¡Gracias por aprender y construir juntos! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
