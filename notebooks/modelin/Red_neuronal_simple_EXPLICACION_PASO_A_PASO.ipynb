{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732fb3a5",
   "metadata": {},
   "source": [
    "# Proyecto de Machine Learning de Extremo a Extremo - Parte I\n",
    "\n",
    "Este cuaderno es la primera parte de un proyecto completo de Machine Learning.\n",
    "AquÃ­ aprenderemos a detectar comentarios tÃ³xicos usando cÃ³digo y explicaciones sencillas.\n",
    "\n",
    "## Â¿QuÃ© son los Proyectos de Machine Learning (ML) de Extremo a Extremo?\n",
    "Imagina que quieres construir un robot que pueda reconocer si una flor es roja o azul.\n",
    "Un proyecto de Machine Learning de extremo a extremo es como hacer todo el trabajo para ese robot, desde enseÃ±arle a mirar las flores hasta ponerlo a trabajar en tu jardÃ­n para que las identifique.\n",
    "Cubre cada paso, desde que tienes una idea hasta que el robot estÃ¡ funcionando de verdad.\n",
    "Estos proyectos son muy completos y te enseÃ±an todo lo que necesitas saber sobre cÃ³mo hacer que una computadora aprenda.\n",
    "\n",
    "## Â¿Por quÃ© son necesarios los Proyectos de ML de Extremo a Extremo?\n",
    "- Aprendes de forma completa: Ves todo el camino, desde que juntas la informaciÃ³n (las fotos de las flores) hasta que el robot estÃ¡ listo para usar.\n",
    "- Desarrollas habilidades importantes: Te vuelves bueno resolviendo problemas, usando herramientas especiales y organizando tu trabajo.\n",
    "- Creas soluciones reales: Tu robot puede ayudar en la vida real, Â¡identificando flores para ti!\n",
    "- Construyes un portafolio fuerte: Si quieres trabajar con esto, puedes mostrar lo que has hecho.\n",
    "- Ayudas a cumplir objetivos: Si una empresa quiere que un robot clasifique flores, este proyecto asegura que el robot realmente haga lo que la empresa necesita.\n",
    "\n",
    "En esta primera parte, nos enfocaremos en hacer experimentos en este cuaderno, como si fuera nuestro laboratorio.\n",
    "\n",
    "---\n",
    "\n",
    "## PARTE I: ConfiguraciÃ³n del Cuaderno\n",
    "Empezaremos trayendo todas las herramientas que necesitamos. Piensa en esto como preparar tu caja de herramientas antes de empezar a construir algo.\n",
    "\n",
    "### ImportaciÃ³n de LibrerÃ­as Esenciales\n",
    "Este cÃ³digo nos ayuda a traer las herramientas que vamos a necesitar para nuestro proyecto.\n",
    "\n",
    "**Ãndice de Pasos en este CÃ³digo:**\n",
    "1. Importar herramientas para nÃºmeros (NumPy).\n",
    "2. Importar herramientas para tablas de datos (Pandas).\n",
    "3. Importar herramientas para dibujar grÃ¡ficos (Matplotlib y Seaborn).\n",
    "4. Importar herramientas para trabajar con texto especial (re).\n",
    "5. Importar herramientas para crear nubes de palabras (WordCloud).\n",
    "6. Importar herramientas para construir modelos de aprendizaje (Scikit-learn).\n",
    "7. Importar herramientas para redes neuronales (TensorFlow y Keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Esto es como traer una calculadora superpotente al cuaderno. La usaremos para hacer cÃ¡lculos grandes con nÃºmeros, como sumar muchas cosas a la vez. Es muy Ãºtil cuando tenemos muchos datos.\n",
    "import pandas as pd # Esto es como traer una libreta mÃ¡gica donde podemos organizar informaciÃ³n en tablas, como si fueran hojas de cÃ¡lculo. Es perfecta para guardar y ordenar los comentarios y para saber si son tÃ³xicos o no. La usaremos mucho para ver nuestros datos.\n",
    "from matplotlib import pyplot as plt # Esto es como traer lÃ¡pices de colores y papel para dibujar. Nos ayudarÃ¡ a hacer dibujos bonitos con nuestros datos, como barras o lÃ­neas, para entenderlos mejor. AsÃ­ podemos ver si hay muchos comentarios tÃ³xicos o pocos.\n",
    " # Este es un truco especial para que los dibujos que hagamos con Matplotlib aparezcan justo aquÃ­, en nuestro cuaderno, sin tener que ir a otro lugar a verlos. Es como dibujar directamente en nuestra hoja de trabajo.\n",
    "import seaborn as sns # Esto es como traer otro juego de lÃ¡pices y pinceles para dibujar, pero aÃºn mÃ¡s bonitos. Seaborn nos ayuda a hacer grÃ¡ficos mÃ¡s elaborados y con colores mÃ¡s agradables que Matplotlib, haciendo que nuestros datos se vean mejor.\n",
    "import re # Esto es como traer una lupa mÃ¡gica para buscar y cambiar letras o palabras en los comentarios. Nos sirve para limpiar el texto y quitar cosas que no necesitamos, como signos de puntuaciÃ³n extraÃ±os o errores.\n",
    "from wordcloud import WordCloud # Esto es como traer un juguete que hace nubes con palabras. Con esto, podemos ver las palabras mÃ¡s grandes y populares en los comentarios, como si fueran nubes formadas por palabras. Las palabras mÃ¡s usadas aparecerÃ¡n mÃ¡s grandes.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Esta es una herramienta de un kit de construcciÃ³n (Scikit-learn) que nos ayuda a convertir las palabras en nÃºmeros. Imagina que cada palabra tiene un nÃºmero secreto y esta herramienta lo descubre para que la computadora pueda entenderlas y aprender de ellas.\n",
    "from sklearn.model_selection import train_test_split # Esta es otra herramienta del kit que nos ayuda a dividir nuestros datos. Es como si tuviÃ©ramos un montÃ³n de juguetes y los separamos en dos grupos: uno para que nuestro modelo practique (entrenamiento) y otro para ver si lo que aprendiÃ³ funciona bien con juguetes que nunca ha visto (prueba).\n",
    "from sklearn.metrics import classification_report # Esta herramienta del kit nos ayuda a saber quÃ© tan bien le fue a nuestro modelo. Es como un boletÃ­n de calificaciones para nuestro modelo, diciÃ©ndonos si hizo un buen trabajo o si necesita mejorar al clasificar los comentarios.\n",
    "import tensorflow as tf # Esto es como traer un cerebro gigante al cuaderno. TensorFlow es una herramienta muy avanzada que nos permite construir \"cerebritos\" artificiales, llamados redes neuronales, que aprenden a reconocer patrones, como si un comentario es tÃ³xico. Es el motor principal de nuestro modelo.\n",
    "from tensorflow.keras.models import Sequential # Esto es como elegir un tipo de caja para construir nuestro cerebrito artificial. \"Sequential\" significa que el cerebrito va a aprender paso a paso, como una cadena donde la informaciÃ³n fluye de una capa a la siguiente.\n",
    "from tensorflow.keras.layers import Dense, Dropout # Estas son las piezas que ponemos dentro de la caja de nuestro cerebrito.\n",
    "# 'Dense' es como una capa de neuronas (pequeÃ±as celdas que aprenden) donde cada neurona estÃ¡ conectada con todas las neuronas de la capa anterior. Es como si todas las ideas de un paso se compartieran con el siguiente, ayudando al modelo a entender relaciones complejas.\n",
    "# 'Dropout' es una pieza que ayuda a que el cerebrito no \"memorice\" demasiado. Imagina que a veces, algunas neuronas se toman un descanso (se apagan un rato), para que el cerebrito aprenda de muchas maneras diferentes y no se vuelva flojo. Esto previene que el modelo se \"sobreajuste\" a los datos de entrenamiento.\n",
    "from tensorflow.keras.optimizers import Adam # Esto es como el entrenador de nuestro cerebrito. Adam es un tipo especial de entrenador que le enseÃ±a al cerebrito cÃ³mo aprender mÃ¡s rÃ¡pido y mejor, ajustando las conexiones entre las neuronas para que cometa menos errores y mejore su precisiÃ³n con el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ec74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72be21",
   "metadata": {},
   "source": [
    "### Herramientas y su funciÃ³n\n",
    "\n",
    "| Herramienta | FunciÃ³n Principal | Â¿Para quÃ© sirve? |\n",
    "|-------------|------------------|------------------|\n",
    "| numpy | CÃ¡lculos numÃ©ricos y arreglos | Realizar operaciones matemÃ¡ticas eficientes con grandes conjuntos de datos, como sumar miles de nÃºmeros. |\n",
    "| pandas | ManipulaciÃ³n y anÃ¡lisis de datos | Organizar datos en tablas (como hojas de cÃ¡lculo) para facilitar su lectura, limpieza y anÃ¡lisis. |\n",
    "| matplotlib.pyplot | VisualizaciÃ³n de datos bÃ¡sica | Crear grÃ¡ficos simples como barras o lÃ­neas para ver cÃ³mo se distribuyen los datos. |\n",
    "| %matplotlib inline | IntegraciÃ³n de grÃ¡ficos en Jupyter | Hace que los grÃ¡ficos que dibujamos aparezcan justo aquÃ­, en nuestro cuaderno. |\n",
    "| seaborn | VisualizaciÃ³n de datos avanzada | Crear grÃ¡ficos estadÃ­sticos mÃ¡s complejos y bonitos para explorar relaciones entre variables. |\n",
    "| re | Expresiones regulares (texto) | Buscar, reemplazar y manipular patrones de texto de forma avanzada, Ãºtil para limpiar comentarios. |\n",
    "| WordCloud | VisualizaciÃ³n de nubes de palabras | Generar imÃ¡genes donde las palabras mÃ¡s usadas aparecen mÃ¡s grandes. |\n",
    "| TfidfVectorizer | VectorizaciÃ³n de texto (TF-IDF) | Convertir las palabras de los comentarios en nÃºmeros (vectores) que los modelos de ML pueden entender. |\n",
    "| train_test_split | DivisiÃ³n de datos | Separar nuestros datos en dos grupos: uno para que el modelo aprenda y otro para ver si aprendiÃ³ bien. |\n",
    "| classification_report | EvaluaciÃ³n del modelo | Generar un informe detallado que nos dice quÃ© tan bien el modelo clasifica los comentarios. |\n",
    "| tensorflow | Plataforma de ML/DL | Proporcionar las herramientas para construir y entrenar redes neuronales profundas. |\n",
    "| Sequential | Tipo de modelo Keras | Definir nuestro \"cerebro\" artificial (red neuronal) de una manera sencilla. |\n",
    "| Dense | Capa de red neuronal | Son las \"neuronas\" de nuestro \"cerebro\". Cada una aprende algo y pasa su conocimiento a la siguiente capa. |\n",
    "| Dropout | RegularizaciÃ³n de red neuronal | Desactivar aleatoriamente algunas \"neuronas\" durante el entrenamiento para que el \"cerebro\" no se vuelva perezoso. |\n",
    "| Adam | Optimizador de red neuronal | Es el \"entrenador\" de nuestro \"cerebro\". Le enseÃ±a cÃ³mo ajustar sus conexiones para cometer menos errores. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2ad59",
   "metadata": {},
   "source": [
    "### Cuento para niÃ±os: Las Herramientas del Detective de Palabras MÃ¡gicas\n",
    "\n",
    "En un pueblo muy especial, la gente se comunicaba con palabras que aparecÃ­an en una pantalla mÃ¡gica. La mayorÃ­a de las palabras eran brillantes y amables, pero a veces, aparecÃ­an palabras con pequeÃ±as espinas, que hacÃ­an que los demÃ¡s se sintieran tristes o molestos. Esas eran las \"palabras tÃ³xicas\".\n",
    "\n",
    "Un dÃ­a, el Detective de Palabras MÃ¡gicas decidiÃ³ que querÃ­a ayudar a mantener el pueblo feliz. Para eso, necesitaba construir un asistente muy inteligente que pudiera encontrar las palabras tÃ³xicas. AbriÃ³ su gran caja de herramientas, Â¡y esto es lo que encontrÃ³!\n",
    "\n",
    "(Lee el cuento completo en la celda original para mÃ¡s detalles sobre cada herramienta y su funciÃ³n.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d950d",
   "metadata": {},
   "source": [
    "### AÃ±adiendo Conjuntos de Datos\n",
    "\n",
    "Usaremos dos colecciones de comentarios para este proyecto:\n",
    "1. DesafÃ­o de ClasificaciÃ³n de Comentarios TÃ³xicos (Â¡una colecciÃ³n grande!)\n",
    "2. Comentarios tÃ³xicos de Youtube (Â¡una colecciÃ³n mÃ¡s pequeÃ±a pero tambiÃ©n Ãºtil!)\n",
    "\n",
    "Vamos a aÃ±adir estos datos a nuestro cuaderno. Imagina que son dos cajas llenas de notas con comentarios.\n",
    "\n",
    "*(AquÃ­ irÃ­a la imagen que mencionas en el texto original, pero como no puedo generar imÃ¡genes, se omite.)*\n",
    "\n",
    "Ahora, vamos a importar (traer) estos conjuntos de datos a nuestras tablas de `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ­tulo del Bloque de CÃ³digo: Cargar y Combinar Conjuntos de Datos de Comentarios\n",
    "\n",
    "# Ãndice de Pasos en este CÃ³digo:\n",
    "# 1. Cargar el primer conjunto de datos (comentarios de Kaggle).\n",
    "# 2. Cargar el segundo conjunto de datos (comentarios de YouTube).\n",
    "# 3. Procesar el primer conjunto de datos para que tenga solo el texto y si es tÃ³xico.\n",
    "# 4. Procesar el segundo conjunto de datos para que tenga solo el texto y si es tÃ³xico.\n",
    "# 5. Unir los dos conjuntos de datos procesados en una sola tabla grande.\n",
    "# 6. Mostrar las primeras filas de la tabla combinada para ver cÃ³mo quedÃ³.\n",
    "\n",
    "# Paso 1: Cargar el primer conjunto de datos (comentarios de Kaggle)\n",
    "df1 = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n",
    "\n",
    "# Paso 2: Cargar el segundo conjunto de datos (comentarios de YouTube)\n",
    "df2 = pd.read_csv('/kaggle/input/youtube-toxicity-data/youtoxic_english_1000.csv')\n",
    "\n",
    "# Paso 3: Procesar df1 (el primer conjunto de datos)\n",
    "df1['Toxic'] = df1.iloc[:, 2:].any(axis=1)\n",
    "df1_processed = df1[['comment_text', 'Toxic']].rename(columns={'comment_text': 'Text'})\n",
    "\n",
    "# Paso 4: Procesar df2 (el segundo conjunto de datos)\n",
    "df2['Toxic'] = df2.iloc[:, 3:].any(axis=1)\n",
    "df2_processed = df2[['Text', 'Toxic']]\n",
    "\n",
    "# Paso 5: Combinar df1_processed y df2_processed\n",
    "df = pd.concat([df1_processed, df2_processed], ignore_index=True)\n",
    "\n",
    "# Paso 6: Mostrar las primeras filas de final_df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583cbea",
   "metadata": {},
   "source": [
    "### Herramientas clave en la carga y combinaciÃ³n de datos\n",
    "\n",
    "| Herramienta | FunciÃ³n Principal | Â¿Para quÃ© sirve? |\n",
    "|-------------|------------------|------------------|\n",
    "| pd.read_csv() | Cargar datos de archivos CSV | Traer la informaciÃ³n de los comentarios que tenemos guardados en archivos a nuestro cuaderno para poder trabajar con ellos. |\n",
    "| df.iloc[:, 2:].any(axis=1) | Simplificar etiquetas de toxicidad | Unificar varias categorÃ­as de toxicidad en una sola etiqueta \"TÃ³xico\" (Verdadero/Falso). |\n",
    "| df[['comment_text', 'Toxic']] | Seleccionar columnas especÃ­ficas | Quedarnos solo con las columnas importantes que necesitamos: el texto del comentario y su nueva etiqueta de toxicidad. |\n",
    "| .rename(columns={'comment_text': 'Text'}) | Renombrar columnas | Cambiar el nombre de una columna para que sea mÃ¡s fÃ¡cil de entender o para que coincida con otros datos. |\n",
    "| pd.concat([...], ignore_index=True) | Combinar tablas de datos | Unir dos o mÃ¡s tablas de comentarios en una sola tabla grande para tener toda la informaciÃ³n junta. |\n",
    "| df.head() | Visualizar las primeras filas | Mostrar las primeras filas de la tabla combinada para comprobar que la uniÃ³n se hizo correctamente. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611c49f",
   "metadata": {},
   "source": [
    "### Cuento para niÃ±os: Uniendo las Cajas de Comentarios\n",
    "\n",
    "Imagina que nuestro Detective de Palabras MÃ¡gicas tiene dos cajas llenas de notas. Una caja es muy grande y tiene Notas de DesafÃ­o (el primer dataset), y la otra es mÃ¡s pequeÃ±a y tiene Notas de YouTube (el segundo dataset). Dentro de cada nota, hay un comentario.\n",
    "\n",
    "El Detective querÃ­a juntar todas las notas en una sola caja gigante para que su asistente inteligente aprendiera de todas a la vez.\n",
    "\n",
    "Primero, el Detective usÃ³ su \"Cargador de Cajas\" (pd.read_csv). Con Ã©l, abriÃ³ la Caja de DesafÃ­o y la Caja de YouTube, y las puso sobre la mesa, listas para trabajar.\n",
    "\n",
    "Pero las notas de la Caja de DesafÃ­o eran un poco complicadas. Algunas decÃ­an \"esta palabra es fea\", otras \"esta palabra es grosera\", y otras \"esta palabra amenaza\". El Detective pensÃ³: \"Â¡Uhm, demasiadas etiquetas! Mejor que solo diga si es 'espinosa' o no\". AsÃ­ que usÃ³ su \"Simplificador de Etiquetas\" (.iloc[:, 2:].any(axis=1)). Este simplificador miraba todas esas etiquetas y, si alguna era \"sÃ­\", ponÃ­a un gran \"SÃ, ES ESPINOSA\" en la nota. Si todas eran \"no\", entonces ponÃ­a \"NO ES ESPINOSA\".\n",
    "\n",
    "AdemÃ¡s, las notas de la Caja de DesafÃ­o tenÃ­an una parte que decÃ­a \"Texto del Comentario\", y las de la Caja de YouTube solo decÃ­an \"Texto\". Para que todo fuera igual, el Detective usÃ³ su \"Renombrador MÃ¡gico\" (.rename(columns={'comment_text': 'Text'})) para que en todas las notas, la parte del comentario se llamara simplemente \"Texto\". Â¡AsÃ­ era mÃ¡s ordenado!\n",
    "\n",
    "Una vez que todas las notas estaban simplificadas y con nombres iguales, el Detective usÃ³ su \"Pegamento de Cajas\" (pd.concat). Con este pegamento especial, Â¡uniÃ³ todas las notas de la Caja de DesafÃ­o y la Caja de YouTube en una sola Caja Gigante de Comentarios! AsegurÃ¡ndose de que cada nota tuviera un nÃºmero Ãºnico para que no se mezclaran.\n",
    "\n",
    "Finalmente, para ver si todo habÃ­a quedado bien, el Detective mirÃ³ las primeras cinco notas de la Caja Gigante (df.head()). Â¡Y sÃ­! Todo estaba perfecto, con cada comentario y su etiqueta de \"espinoso\" o \"no espinoso\". Ahora, el asistente inteligente tenÃ­a una gran colecciÃ³n de notas para empezar a aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22eec1d",
   "metadata": {},
   "source": [
    "### Explorando y Limpiando los Datos\n",
    "\n",
    "Ahora que tenemos nuestro conjunto de datos combinado, vamos a hacer lo que a la mayorÃ­a de los cientÃ­ficos de datos les gusta hacer con los datos: Â¡jugar con ellos!\n",
    "\n",
    "A continuaciÃ³n, realizaremos un resumen estadÃ­stico, revisaremos tipos de datos, eliminaremos duplicados y veremos la distribuciÃ³n de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877eb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ­tulo del Bloque de CÃ³digo: ExploraciÃ³n y Limpieza Inicial de Datos\n",
    "\n",
    "# 1. Obtener un resumen estadÃ­stico de los datos\n",
    "print(df.describe())\n",
    "\n",
    "# 2. Verificar tipos de datos y valores faltantes\n",
    "print(df.dtypes)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Verificar comentarios duplicados\n",
    "print(\"Duplicate rows based on 'Text' column:\")\n",
    "duplicate_rows = df[df.duplicated(subset=['Text'], keep=False)]\n",
    "print(duplicate_rows)\n",
    "\n",
    "# 4. Eliminar comentarios duplicados\n",
    "df.drop_duplicates(subset=['Text'], keep='first', inplace=True)\n",
    "\n",
    "# 5. Confirmar que los duplicados fueron eliminados y reindexar\n",
    "print(\"Number of rows after removing duplicates:\", len(df))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 6. DistribuciÃ³n de la columna 'Toxic'\n",
    "toxic_distribution = df['Toxic'].value_counts()\n",
    "print(toxic_distribution)\n",
    "\n",
    "# 7. Guardar nuestra tabla limpia en un nuevo archivo CSV\n",
    "df.to_csv(\"Toxic_Comments_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee874a7",
   "metadata": {},
   "source": [
    "### Herramientas clave en la limpieza y exploraciÃ³n de datos\n",
    "\n",
    "| Herramienta | FunciÃ³n Principal | Â¿Para quÃ© sirve? |\n",
    "|-------------|------------------|------------------|\n",
    "| df.describe() | Resumen estadÃ­stico | Obtener rÃ¡pidamente informaciÃ³n sobre la cantidad de datos, promedios, etc. |\n",
    "| df.dtypes | Comprobar tipos de datos | Verificar si las columnas tienen el tipo de informaciÃ³n correcto. |\n",
    "| df.isnull().sum() | Contar valores nulos | Saber cuÃ¡ntas \"casillas vacÃ­as\" o datos faltantes hay en cada columna. |\n",
    "| df.duplicated() y drop_duplicates() | Identificar y eliminar duplicados | Encontrar comentarios que estÃ¡n repetidos y borrarlos. |\n",
    "| df.reset_index() | Reindexar el DataFrame | Volver a numerar las filas de nuestra tabla de datos de forma consecutiva. |\n",
    "| df['Toxic'].value_counts() | Contar la distribuciÃ³n de clases | Saber cuÃ¡ntos comentarios son tÃ³xicos y cuÃ¡ntos no lo son. |\n",
    "| df.to_csv() | Guardar datos en archivo CSV | Guardar la tabla de datos ya limpia y procesada en un nuevo archivo. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08b2d0",
   "metadata": {},
   "source": [
    "### Limpiando el Texto de los Comentarios\n",
    "Ahora que tenemos los datos limpios y sin duplicados, es hora de limpiar el texto de los comentarios. Esto significa quitar cosas que no ayudan a nuestro modelo a aprender, como signos de puntuaciÃ³n, nÃºmeros, o palabras muy comunes que no aportan mucho significado (como \"el\", \"la\", \"de\").\n",
    "TambiÃ©n convertiremos todo a minÃºsculas para que la computadora no piense que \"Hola\" y \"hola\" son palabras diferentes.\n",
    "\n",
    "#### Â¿Por quÃ© limpiar el texto?\n",
    "Imagina que tu asistente inteligente estÃ¡ aprendiendo a leer. Si le das libros llenos de garabatos, errores y palabras repetidas, Â¡le costarÃ¡ mucho aprender! Limpiar el texto es como darle libros bien escritos y ordenados.\n",
    "\n",
    "A continuaciÃ³n, crearemos una funciÃ³n para limpiar el texto y la aplicaremos a todos los comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34afe42",
   "metadata": {},
   "source": [
    "### Limpieza de Texto: Â¿Por quÃ© es importante?\n",
    "\n",
    "Los comentarios pueden tener errores, sÃ­mbolos raros, mayÃºsculas, emojis, URLs, menciones y muchas cosas que no ayudan a nuestro modelo a aprender. Limpiar el texto es como preparar los ingredientes antes de cocinar: si quitamos lo que no sirve, el platillo (nuestro modelo) saldrÃ¡ mucho mejor.\n",
    "\n",
    "**Â¿QuÃ© vamos a limpiar?**\n",
    "- Convertir todo a minÃºsculas (para que 'Hola' y 'hola' sean lo mismo).\n",
    "- Quitar URLs (enlaces a pÃ¡ginas web).\n",
    "- Quitar menciones (@usuario).\n",
    "- Quitar signos de puntuaciÃ³n y caracteres especiales.\n",
    "- Quitar nÃºmeros.\n",
    "- Quitar espacios extra.\n",
    "- (Opcional) Quitar palabras vacÃ­as (stopwords) y lematizar (convertir palabras a su forma base).\n",
    "\n",
    "Â¡Vamos a crear una funciÃ³n mÃ¡gica para limpiar los comentarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeffaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargamos recursos necesarios de NLTK (solo la primera vez)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Creamos la funciÃ³n de limpieza de texto\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    FunciÃ³n para limpiar comentarios de texto.\n",
    "    - Convierte a minÃºsculas\n",
    "    - Elimina URLs, menciones, signos de puntuaciÃ³n, nÃºmeros y espacios extra\n",
    "    - Elimina stopwords y lematiza las palabras\n",
    "    \"\"\"\n",
    "    # Convertir a minÃºsculas\n",
    "    texto = texto.lower()\n",
    "    # Eliminar URLs\n",
    "    texto = re.sub(r'http\\S+|www\\S+', '', texto)\n",
    "    # Eliminar menciones (@usuario)\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Eliminar signos de puntuaciÃ³n\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Eliminar nÃºmeros\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "    # Eliminar caracteres especiales y tildes\n",
    "    texto = ''.join(c for c in unicodedata.normalize('NFD', texto)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "    # Eliminar espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    # Eliminar stopwords y lematizar\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    palabras = texto.split()\n",
    "    palabras = [lemmatizer.lemmatize(palabra) for palabra in palabras if palabra not in stop_words]\n",
    "    texto_limpio = ' '.join(palabras)\n",
    "    return texto_limpio\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ejemplo = \"@usuario Â¡Hola! Visita https://www.ejemplo.com para mÃ¡s info. #NLP 2023\"\n",
    "print(\"Original:\", ejemplo)\n",
    "print(\"Limpio:\", limpiar_texto(ejemplo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdf40e",
   "metadata": {},
   "source": [
    "### Aplicando la funciÃ³n de limpieza a todos los comentarios\n",
    "\n",
    "Ahora que tenemos nuestra funciÃ³n mÃ¡gica, la aplicaremos a todos los comentarios del dataset. Esto puede tardar un poco si hay muchos datos, Â¡pero el resultado serÃ¡ un texto mucho mÃ¡s limpio y fÃ¡cil de analizar!\n",
    "\n",
    "**Â¿QuÃ© logramos con esto?**\n",
    "- Reducimos el ruido en los datos.\n",
    "- Hacemos que el modelo aprenda mejor.\n",
    "- Preparamos el texto para la siguiente etapa: vectorizaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que el DataFrame combinado se llama 'df' y la columna de texto es 'comment_text'\n",
    "# Si tu columna tiene otro nombre, cÃ¡mbialo aquÃ­\n",
    "\n",
    "df['comment_text_limpio'] = df['comment_text'].apply(limpiar_texto)\n",
    "\n",
    "# Veamos algunos ejemplos antes y despuÃ©s\n",
    "df[['comment_text', 'comment_text_limpio']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad0722",
   "metadata": {},
   "source": [
    "| FunciÃ³n / Herramienta         | Â¿Para quÃ© sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `re.sub`                     | Buscar y reemplazar patrones en texto (regex)                  | Quitar URLs, menciones          |\n",
    "| `str.lower()`                | Convertir texto a minÃºsculas                                   | 'Hola' â†’ 'hola'                 |\n",
    "| `str.translate`              | Eliminar signos de puntuaciÃ³n                                  | 'Â¡Hola!' â†’ 'Hola'               |\n",
    "| `unicodedata.normalize`      | Quitar tildes y caracteres especiales                          | 'acciÃ³n' â†’ 'accion'             |\n",
    "| `stopwords.words`            | Lista de palabras vacÃ­as (stopwords)                           | 'the', 'and', 'is', ...         |\n",
    "| `WordNetLemmatizer`          | Lematizar palabras (forma base)                                | 'running' â†’ 'run'               |\n",
    "| `apply` de pandas            | Aplicar una funciÃ³n a cada elemento de una columna             | Limpiar todos los comentarios    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269f76e",
   "metadata": {},
   "source": [
    "#### Cuento: \"La escoba mÃ¡gica de los comentarios\"\n",
    "\n",
    "HabÃ­a una vez un reino donde los comentarios eran muy desordenados. Algunos tenÃ­an palabras raras, otros estaban llenos de sÃ­mbolos y algunos hablaban en mayÃºsculas todo el tiempo. Los sabios del reino querÃ­an entender quÃ© decÃ­an los comentarios, pero no podÃ­an porque habÃ­a mucho ruido.\n",
    "\n",
    "Un dÃ­a, llegÃ³ una escoba mÃ¡gica llamada `limpiar_texto`. Esta escoba barrÃ­a todo lo que no servÃ­a: URLs, menciones, signos de puntuaciÃ³n, nÃºmeros y hasta palabras que no decÃ­an nada importante. Cuando los sabios usaron la escoba mÃ¡gica, los comentarios quedaron limpios y ordenados, listos para ser analizados por el gran orÃ¡culo (el modelo de machine learning).\n",
    "\n",
    "Desde entonces, cada vez que alguien querÃ­a entender los comentarios, primero usaba la escoba mÃ¡gica. Y asÃ­, el reino pudo descubrir cuÃ¡les comentarios eran tÃ³xicos y cuÃ¡les no, Â¡y todos vivieron mÃ¡s felices y menos ofendidos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff97637",
   "metadata": {},
   "source": [
    "### VectorizaciÃ³n de Texto: Â¿CÃ³mo convertimos palabras en nÃºmeros?\n",
    "\n",
    "Las computadoras no entienden palabras, Â¡entienden nÃºmeros! Para que nuestro modelo pueda aprender de los comentarios, necesitamos transformar el texto en una matriz de nÃºmeros.\n",
    "\n",
    "**Â¿CÃ³mo lo hacemos?**\n",
    "Usaremos una tÃ©cnica llamada **TF-IDF (Term Frequency - Inverse Document Frequency)**. Esta tÃ©cnica nos ayuda a saber quÃ© palabras son importantes en cada comentario, dÃ¡ndoles un peso especial segÃºn cuÃ¡ntas veces aparecen y si son raras o comunes en el conjunto de datos.\n",
    "\n",
    "**Ventajas de TF-IDF:**\n",
    "- Da mÃ¡s importancia a palabras Ãºnicas de cada comentario.\n",
    "- Reduce el peso de palabras muy comunes.\n",
    "- Es fÃ¡cil de usar y funciona bien para muchos problemas de texto.\n",
    "\n",
    "Â¡Vamos a vectorizar nuestros comentarios limpios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creamos el vectorizador TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Puedes ajustar el nÃºmero de caracterÃ­sticas\n",
    "\n",
    "# Ajustamos y transformamos los comentarios limpios\n",
    "tfidf_matrix = tfidf.fit_transform(df['comment_text_limpio'])\n",
    "\n",
    "# Convertimos la matriz a un DataFrame para verla mejor\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126db74",
   "metadata": {},
   "source": [
    "| FunciÃ³n / Herramienta         | Â¿Para quÃ© sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `TfidfVectorizer`            | Convierte texto en una matriz de pesos TF-IDF                  | Vectorizar comentarios          |\n",
    "| `fit_transform`              | Ajusta el vectorizador y transforma el texto                   | Crear matriz numÃ©rica           |\n",
    "| `get_feature_names_out`      | Obtiene los nombres de las palabras (features)                 | Ver columnas del DataFrame      |\n",
    "| `pd.DataFrame`               | Convierte la matriz a un DataFrame de pandas                   | Visualizar la matriz            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e16a9",
   "metadata": {},
   "source": [
    "#### Cuento: \"El traductor de palabras mÃ¡gicas\"\n",
    "\n",
    "En el reino de los comentarios limpios, los sabios querÃ­an que el orÃ¡culo (la computadora) entendiera los mensajes. Pero el orÃ¡culo solo hablaba en nÃºmeros, no en palabras.\n",
    "\n",
    "Entonces, inventaron un traductor mÃ¡gico llamado **TF-IDF**. Este traductor tomaba cada palabra y le daba un nÃºmero especial: si la palabra era muy comÃºn, recibÃ­a un nÃºmero pequeÃ±o; si era rara y especial, recibÃ­a un nÃºmero grande.\n",
    "\n",
    "AsÃ­, cada comentario se transformÃ³ en una fila de nÃºmeros, y el orÃ¡culo pudo empezar a aprender y a predecir cuÃ¡les comentarios eran tÃ³xicos y cuÃ¡les no. Â¡El reino estaba cada vez mÃ¡s cerca de resolver el misterio de la toxicidad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d635d6",
   "metadata": {},
   "source": [
    "# ADVERTENCIA DE ALGO QUE NO SE DEBE HACER \n",
    "\n",
    "No es lo ideal vectorizar y tokenizar **antes** de dividir en train y test. Te explico por quÃ©:\n",
    "\n",
    "### Â¿Por quÃ© es un error vectorizar antes de dividir?\n",
    "Cuando aplicas `TfidfVectorizer` (o cualquier vectorizador/tokenizador) **antes** de dividir los datos, el vectorizador \"ve\" todo el texto, incluyendo los datos de test. Esto significa que:\n",
    "- El vocabulario y los pesos TF-IDF se calculan usando informaciÃ³n de los datos de test.\n",
    "- El modelo puede aprender palabras o patrones que solo existen en el test, lo que genera **fugas de informaciÃ³n** (data leakage).\n",
    "- Las mÃ©tricas de evaluaciÃ³n serÃ¡n demasiado optimistas y no reflejarÃ¡n el rendimiento real en datos nuevos.\n",
    "\n",
    "### Â¿CuÃ¡l es la forma correcta?\n",
    "1. **Divide** primero el dataset en train y test (`train_test_split`).\n",
    "2. **Ajusta** (`fit`) el vectorizador **solo** con los datos de entrenamiento.\n",
    "3. **Transforma** (`transform`) tanto el train como el test usando ese vectorizador ya ajustado.\n",
    "\n",
    "#### Ejemplo correcto:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Divide primero\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Toxic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Ajusta solo con train\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# 3. Transforma test con el mismo vectorizador\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "```\n",
    "\n",
    "### Resumen\n",
    "- **No** vectorices/tokenices antes de dividir.\n",
    "- **SÃ­** divide primero, luego ajusta el vectorizador solo con train, y despuÃ©s transforma ambos conjuntos.\n",
    "\n",
    "Esto evita fugas de informaciÃ³n y asegura una evaluaciÃ³n justa y realista de tu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9c47",
   "metadata": {},
   "source": [
    "### Balanceo de Clases: Â¿QuÃ© pasa si hay pocos comentarios tÃ³xicos?\n",
    "\n",
    "En muchos problemas de clasificaciÃ³n, como la detecciÃ³n de comentarios tÃ³xicos, suele haber muchos mÃ¡s ejemplos de una clase que de otra (por ejemplo, muchos comentarios no tÃ³xicos y pocos tÃ³xicos). Esto puede hacer que el modelo aprenda a ignorar la clase minoritaria.\n",
    "\n",
    "**Â¿CÃ³mo lo solucionamos?**\n",
    "Usaremos una tÃ©cnica llamada **SMOTE (Synthetic Minority Over-sampling Technique)**, que crea ejemplos sintÃ©ticos de la clase minoritaria para balancear el dataset.\n",
    "\n",
    "**Ventajas de SMOTE:**\n",
    "- Ayuda a que el modelo no se sesgue hacia la clase mayoritaria.\n",
    "- Mejora la capacidad de detectar comentarios tÃ³xicos.\n",
    "- Es fÃ¡cil de aplicar despuÃ©s de vectorizar los datos.\n",
    "\n",
    "Â¡Vamos a balancear nuestras clases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Definimos las variables X (caracterÃ­sticas) e y (etiqueta)\n",
    "X = tfidf_df\n",
    "# Suponiendo que la columna de la etiqueta es 'toxic' (ajusta si tu columna tiene otro nombre)\n",
    "y = df['toxic']\n",
    "\n",
    "# Creamos el objeto SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Aplicamos SMOTE para balancear las clases\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Veamos la nueva distribuciÃ³n de clases\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(x=y_res)\n",
    "plt.title('DistribuciÃ³n de clases despuÃ©s de SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88235ee",
   "metadata": {},
   "source": [
    "| FunciÃ³n / Herramienta         | Â¿Para quÃ© sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `SMOTE`                      | Genera ejemplos sintÃ©ticos de la clase minoritaria             | Balancear clases                |\n",
    "| `fit_resample`               | Ajusta y aplica el balanceo a los datos                        | Obtener X_res, y_res            |\n",
    "| `sns.countplot`              | Grafica la distribuciÃ³n de clases                              | Visualizar balanceo             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9fd90",
   "metadata": {},
   "source": [
    "#### Cuento: \"El mago que clonaba comentarios\"\n",
    "\n",
    "En el reino de los comentarios, habÃ­a un problema: los comentarios tÃ³xicos eran muy pocos y los no tÃ³xicos eran muchÃ­simos. El orÃ¡culo (modelo) solo aprendÃ­a de los no tÃ³xicos y se olvidaba de los tÃ³xicos.\n",
    "\n",
    "Un dÃ­a, llegÃ³ un mago llamado **SMOTE**. Este mago tenÃ­a el poder de crear clones mÃ¡gicos de los comentarios tÃ³xicos, para que hubiera la misma cantidad que de los no tÃ³xicos. AsÃ­, el orÃ¡culo pudo aprender de ambos por igual y se volviÃ³ mucho mÃ¡s justo y sabio.\n",
    "\n",
    "Desde entonces, cada vez que habÃ­a un desequilibrio, llamaban al mago SMOTE para que ayudara a balancear el reino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcdc7a",
   "metadata": {},
   "source": [
    "### ConstrucciÃ³n y Entrenamiento del Modelo: Â¡Hora de predecir!\n",
    "\n",
    "Ahora que tenemos nuestros datos limpios, vectorizados y balanceados, Â¡es momento de construir nuestro modelo! Usaremos una **red neuronal simple** para predecir si un comentario es tÃ³xico o no.\n",
    "\n",
    "**Â¿Por quÃ© una red neuronal?**\n",
    "- Puede aprender patrones complejos en los datos.\n",
    "- Es flexible y se adapta a diferentes problemas.\n",
    "- Aunque es simple, puede lograr buenos resultados en tareas de texto.\n",
    "\n",
    "Â¡Vamos a construir y entrenar nuestro modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4334ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definimos la arquitectura de la red neuronal\n",
    "modelo = Sequential()\n",
    "modelo.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "modelo.add(Dropout(0.5))\n",
    "modelo.add(Dense(64, activation='relu'))\n",
    "modelo.add(Dropout(0.3))\n",
    "modelo.add(Dense(1, activation='sigmoid'))  # Salida binaria\n",
    "\n",
    "# Compilamos el modelo\n",
    "modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Definimos early stopping para evitar sobreajuste\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "historial = modelo.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80294a",
   "metadata": {},
   "source": [
    "| FunciÃ³n / Herramienta         | Â¿Para quÃ© sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `Sequential`                 | Crear un modelo de red neuronal secuencial                     | Definir la arquitectura         |\n",
    "| `Dense`                      | Capa densa (totalmente conectada)                              | AÃ±adir capas al modelo          |\n",
    "| `Dropout`                    | Evitar sobreajuste eliminando neuronas aleatoriamente          | RegularizaciÃ³n                  |\n",
    "| `compile`                    | Configurar el modelo (optimizador, funciÃ³n de pÃ©rdida, mÃ©trica)| Preparar para entrenar          |\n",
    "| `fit`                        | Entrenar el modelo con los datos                               | Ajustar pesos                   |\n",
    "| `EarlyStopping`              | Detener el entrenamiento si no mejora                          | Evitar sobreajuste              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855f6e2",
   "metadata": {},
   "source": [
    "#### Cuento: \"El aprendiz que aprendÃ­a de ejemplos\"\n",
    "\n",
    "En el reino, los sabios construyeron un aprendiz especial llamado **Red Neuronal**. Este aprendiz tenÃ­a muchas neuronas (pequeÃ±os ayudantes) que se pasaban mensajes entre sÃ­ para aprender a distinguir comentarios tÃ³xicos de los que no lo eran.\n",
    "\n",
    "Cada vez que el aprendiz veÃ­a un ejemplo, ajustaba sus conexiones para mejorar. Si se equivocaba, aprendÃ­a del error y lo intentaba de nuevo. AsÃ­, poco a poco, se volviÃ³ muy bueno prediciendo la toxicidad de los comentarios.\n",
    "\n",
    "Y asÃ­, el reino pudo confiar en su aprendiz para mantener la paz y la armonÃ­a en los comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673c49c",
   "metadata": {},
   "source": [
    "### EvaluaciÃ³n del Modelo: Â¿QuÃ© tan bien predice?\n",
    "\n",
    "DespuÃ©s de entrenar nuestro modelo, es importante saber quÃ© tan bien funciona. Para eso, evaluaremos su desempeÃ±o usando el conjunto de prueba y diferentes mÃ©tricas.\n",
    "\n",
    "**Â¿QuÃ© vamos a ver?**\n",
    "- PrecisiÃ³n (accuracy)\n",
    "- Matriz de confusiÃ³n\n",
    "- Curva ROC y AUC\n",
    "- GrÃ¡ficas de la historia de entrenamiento\n",
    "\n",
    "Â¡Vamos a evaluar a nuestro aprendiz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ded245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_prob = modelo.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# PrecisiÃ³n\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"PrecisiÃ³n (accuracy): {acc:.4f}\")\n",
    "\n",
    "# Matriz de confusiÃ³n\n",
    "cmp = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusiÃ³n:\\n\", cmp)\n",
    "\n",
    "# Reporte de clasificaciÃ³n\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Curva ROC y AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# GrÃ¡ficas de la historia de entrenamiento\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(historial.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(historial.history['val_accuracy'], label='ValidaciÃ³n')\n",
    "plt.title('PrecisiÃ³n durante el entrenamiento')\n",
    "plt.xlabel('Ã‰poca')\n",
    "plt.ylabel('PrecisiÃ³n')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(historial.history['loss'], label='Entrenamiento')\n",
    "plt.plot(historial.history['val_loss'], label='ValidaciÃ³n')\n",
    "plt.title('PÃ©rdida durante el entrenamiento')\n",
    "plt.xlabel('Ã‰poca')\n",
    "plt.ylabel('PÃ©rdida')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89157b3",
   "metadata": {},
   "source": [
    "| FunciÃ³n / Herramienta         | Â¿Para quÃ© sirve?                                               | Ejemplo de uso                  |\n",
    "|------------------------------|----------------------------------------------------------------|---------------------------------|\n",
    "| `accuracy_score`             | Calcula la precisiÃ³n del modelo                                | Medir desempeÃ±o                 |\n",
    "| `confusion_matrix`           | Muestra aciertos y errores por clase                           | Ver errores del modelo          |\n",
    "| `classification_report`      | Resumen de mÃ©tricas (precision, recall, f1)                    | Evaluar a fondo                 |\n",
    "| `roc_curve`, `auc`           | Calcular y graficar la curva ROC y el Ã¡rea bajo la curva       | Medir discriminaciÃ³n            |\n",
    "| `plt.plot`                   | Graficar resultados y evoluciÃ³n del entrenamiento              | Visualizar desempeÃ±o            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffea7dd",
   "metadata": {},
   "source": [
    "#### Cuento: \"El examen del aprendiz\"\n",
    "\n",
    "DespuÃ©s de mucho entrenamiento, el aprendiz (red neuronal) tuvo que presentar un examen. Los sabios le dieron comentarios que nunca habÃ­a visto y observaron cÃ³mo respondÃ­a.\n",
    "\n",
    "Algunas veces acertÃ³, otras se equivocÃ³, pero cada resultado fue anotado en una gran pizarra (la matriz de confusiÃ³n). TambiÃ©n midieron quÃ© tan bien distinguÃ­a entre tÃ³xicos y no tÃ³xicos usando una cuerda mÃ¡gica llamada **curva ROC**.\n",
    "\n",
    "Gracias a este examen, los sabios supieron si el aprendiz estaba listo para ayudar en el reino o si necesitaba mÃ¡s prÃ¡ctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07352fe9",
   "metadata": {},
   "source": [
    "### Guardando el Modelo y el Vectorizador: Â¡Para usarlo despuÃ©s!\n",
    "\n",
    "Una vez que tenemos un modelo entrenado y un vectorizador TF-IDF, es muy Ãºtil guardarlos para poder usarlos en el futuro sin tener que volver a entrenar todo desde cero.\n",
    "\n",
    "**Â¿QuÃ© vamos a guardar?**\n",
    "- El modelo de red neuronal (en formato HDF5 o SavedModel de Keras).\n",
    "- El vectorizador TF-IDF (con pickle).\n",
    "\n",
    "Â¡Vamos a guardar nuestro trabajo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el modelo de Keras\n",
    "modelo.save('../models/modelo_red_neuronal.h5')\n",
    "\n",
    "# Guardar el vectorizador TF-IDF\n",
    "with open('../data/processed/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"Modelo y vectorizador guardados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a1ee3",
   "metadata": {},
   "source": [
    "### Usando el Modelo Guardado: Â¡Predice nuevos comentarios!\n",
    "\n",
    "Ahora que tenemos nuestro modelo y vectorizador guardados, podemos cargarlos en cualquier momento y usarlos para predecir si un nuevo comentario es tÃ³xico o no.\n",
    "\n",
    "**Â¿CÃ³mo lo hacemos?**\n",
    "- Cargamos el modelo y el vectorizador.\n",
    "- Limpiamos el nuevo comentario.\n",
    "- Lo transformamos con el vectorizador.\n",
    "- Usamos el modelo para predecir.\n",
    "\n",
    "Â¡Vamos a probarlo con un ejemplo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21859b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Cargar el modelo y el vectorizador\n",
    "tfidf_loaded = None\n",
    "with open('../data/processed/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_loaded = pickle.load(f)\n",
    "modelo_loaded = load_model('../models/modelo_red_neuronal.h5')\n",
    "\n",
    "# Nuevo comentario para predecir\n",
    "nuevo_comentario = \"You are so stupid and ugly!\"\n",
    "\n",
    "# Limpiar el comentario\n",
    "comentario_limpio = limpiar_texto(nuevo_comentario)\n",
    "\n",
    "# Vectorizar\n",
    "comentario_vectorizado = tfidf_loaded.transform([comentario_limpio])\n",
    "\n",
    "# Predecir\n",
    "prediccion = modelo_loaded.predict(comentario_vectorizado)\n",
    "if prediccion[0][0] > 0.5:\n",
    "    print(\"El comentario es TÃ“XICO\")\n",
    "else:\n",
    "    print(\"El comentario NO es tÃ³xico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be6d3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Â¡Felicidades! Has completado un proyecto de Machine Learning de Extremo a Extremo\n",
    "\n",
    "En este cuaderno aprendiste a:\n",
    "- Cargar y combinar datos de diferentes fuentes.\n",
    "- Limpiar y preparar texto para anÃ¡lisis.\n",
    "- Vectorizar texto con TF-IDF.\n",
    "- Balancear clases con SMOTE.\n",
    "- Construir, entrenar y evaluar una red neuronal simple.\n",
    "- Guardar y reutilizar tu modelo y vectorizador.\n",
    "- Predecir la toxicidad de nuevos comentarios.\n",
    "\n",
    "### Recuerda:\n",
    "Cada paso es importante y, aunque aquÃ­ usamos ejemplos sencillos y cuentos, Â¡estos conceptos se aplican en proyectos reales!\n",
    "\n",
    "Sigue practicando, experimenta con otros modelos y datasets, y conviÃ©rtete en un gran detective de palabras mÃ¡gicas.\n",
    "\n",
    "Â¡Gracias por aprender y construir juntos! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
